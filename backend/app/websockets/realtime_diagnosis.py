from fastapi import WebSocket, WebSocketDisconnect
import json
import asyncio
import logging
from datetime import datetime
from typing import Dict, List, Set, Deque
from collections import deque
import time
from ..services.analyzer.turn_fault_analyzer import TurnFaultAnalyzer
from ..services.analyzer.broken_bar_analyzer import BrokenBarAnalyzer
from ..services.analyzer.insulation_analyzer import InsulationAnalyzer
from ..services.analyzer.bearing_analyzer import BearingAnalyzer
from ..services.analyzer.eccentricity_analyzer import EccentricityAnalyzer
import random
import pandas as pd # Added for time_series data handling

# å¯¼å…¥ç®€å•å†…å­˜é˜Ÿåˆ—æœåŠ¡
from ..services.simple_queue import simple_queue, TOPICS

# å¯¼å…¥è½»é‡çº§æ¡¥æ¥ç»„ä»¶
from ..services.redis_stream.stream_to_frontend_bridge import stream_bridge

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger("realtime-diagnosis")

class ConnectionManager:
    """WebSocketè¿æ¥ç®¡ç†å™¨"""
    
    def __init__(self):
        # æŒ‰å®¢æˆ·ç«¯ç±»å‹å­˜å‚¨è¿æ¥
        self.active_connections: Dict[str, List[WebSocket]] = {
            "frontend": [],  # å‰ç«¯è¿æ¥
            "datasource": []  # æ•°æ®æºè¿æ¥
        }
        # è¯Šæ–­å™¨å®ä¾‹
        self.analyzers = {
            "turn_fault": TurnFaultAnalyzer(),
            "broken_bar": BrokenBarAnalyzer(),
            "insulation": InsulationAnalyzer(),
            "bearing": BearingAnalyzer(),
            "eccentricity": EccentricityAnalyzer()
        }
        
        # æ•°æ®å¤„ç†é˜Ÿåˆ— - å¢åŠ å®¹é‡ä»¥é€‚åº”é«˜é¢‘æ•°æ®
        self.data_queue: Deque[dict] = deque(maxlen=1000)  # æœ€å¤šç¼“å­˜1000æ¡æ•°æ®
        
        # å¤„ç†çŠ¶æ€
        self.is_processing = False
        self.processing_task = None
        
        # æ•°æ®å¤„ç†é€Ÿç‡æ§åˆ¶ - æé«˜å¤„ç†èƒ½åŠ›
        self.min_processing_rate = 10   # æœ€å°å¤„ç†é€Ÿç‡ï¼ˆæ¯ç§’ï¼‰
        self.max_processing_rate = 50   # æœ€å¤§å¤„ç†é€Ÿç‡ï¼ˆæ¯ç§’ï¼‰
        self.target_processing_rate = 25  # ç›®æ ‡å¤„ç†é€Ÿç‡ï¼ˆæ¯ç§’ï¼‰
        self.last_process_time = time.time()
        self.processed_count = 0
        
        # æ€§èƒ½ç›‘æ§
        self.processing_times = deque(maxlen=50)  # å­˜å‚¨æœ€è¿‘50æ¬¡å¤„ç†æ—¶é—´
        self.last_rate_adjustment = time.time()
        self.rate_adjustment_interval = 5.0  # æ¯5ç§’è°ƒæ•´ä¸€æ¬¡å¤„ç†é€Ÿç‡
        
        # æ•°æ®é‡‡æ ·æ§åˆ¶
        self.sampling_enabled = True
        self.sampling_rate = 1.0  # åˆå§‹é‡‡æ ·ç‡100%
        self.last_data_source_id = None
        self.data_source_counter = {}
        self.last_data_timestamp = {}
        self.min_time_between_samples = 0.1  # åŒä¸€æ•°æ®æºè‡³å°‘é—´éš”100ms
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            "received_count": 0,       # æ¥æ”¶çš„æ•°æ®åŒ…æ€»æ•°
            "processed_count": 0,      # å¤„ç†çš„æ•°æ®åŒ…æ€»æ•°
            "dropped_count": 0,        # ä¸¢å¼ƒçš„æ•°æ®åŒ…æ€»æ•°
            "sampled_count": 0,        # é‡‡æ ·å¤„ç†çš„æ•°æ®åŒ…æ•°
            "error_count": 0,          # å¤„ç†é”™è¯¯æ¬¡æ•°
            "start_time": time.time(), # å¯åŠ¨æ—¶é—´
            "queue_max_length": 0,     # é˜Ÿåˆ—æœ€å¤§é•¿åº¦
            "avg_processing_time": 0,  # å¹³å‡å¤„ç†æ—¶é—´
            "max_processing_time": 0,  # æœ€å¤§å¤„ç†æ—¶é—´
            "current_sampling_rate": self.sampling_rate, # å½“å‰é‡‡æ ·ç‡
            "current_processing_rate": self.target_processing_rate, # å½“å‰å¤„ç†é€Ÿç‡
            "redis_sent_count": 0,     # å‘é€åˆ°Redisçš„æ¶ˆæ¯æ•°
            "redis_error_count": 0     # Rediså‘é€é”™è¯¯æ¬¡æ•°
        }
        
        # çŠ¶æ€ç›‘æ§ä»»åŠ¡
        self.status_monitor_task = None
        
        # Redisé›†æˆæ ‡å¿—
        self.use_redis = True  # æ˜¯å¦ä½¿ç”¨Redis
        
        # å¯åŠ¨é˜Ÿåˆ—å¤„ç†ä»»åŠ¡
        self.start_queue_processor()
        
        # å¯åŠ¨çŠ¶æ€ç›‘æ§
        self.start_status_monitor()
        
        # Redis Streamæ¡¥æ¥ç»„ä»¶æ ‡å¿—ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
        self.bridge_initialized = False
    
    async def connect(self, websocket: WebSocket, client_type: str):
        """å¤„ç†æ–°çš„WebSocketè¿æ¥"""
        await websocket.accept()
        if client_type not in self.active_connections:
            self.active_connections[client_type] = []
        self.active_connections[client_type].append(websocket)
        logger.info(f"{client_type} å®¢æˆ·ç«¯å·²è¿æ¥ï¼Œå½“å‰è¿æ¥æ•°: {len(self.active_connections[client_type])}")
        
        # ç¡®ä¿é˜Ÿåˆ—å¤„ç†å™¨æ­£åœ¨è¿è¡Œ
        self.start_queue_processor()
        # ç¡®ä¿çŠ¶æ€ç›‘æ§æ­£åœ¨è¿è¡Œ
        self.start_status_monitor()
        
        # å¯åŠ¨Redis Streamæ¡¥æ¥ç»„ä»¶ï¼ˆä»…åœ¨å‰ç«¯è¿æ¥ä¸”æœªåˆå§‹åŒ–æ—¶ï¼‰
        if client_type == "frontend" and not self.bridge_initialized:
            # æ£€æŸ¥å…¨å±€æ¡¥æ¥å™¨æ˜¯å¦å·²åˆå§‹åŒ–
            try:
                from ..services.redis_stream.stream_to_frontend_bridge import stream_bridge
                if stream_bridge.redis_client is not None:
                    # å…¨å±€æ¡¥æ¥å™¨å·²åˆå§‹åŒ–ï¼Œæ ‡è®°ä¸ºå·²åˆå§‹åŒ–çŠ¶æ€
                    self.bridge_initialized = True
                    logger.info("ğŸ”— æ£€æµ‹åˆ°å…¨å±€Redis Streamæ¡¥æ¥å™¨å·²åˆå§‹åŒ–ï¼Œè·³è¿‡é‡å¤åˆå§‹åŒ–")
                else:
                    # éœ€è¦åˆå§‹åŒ–æ¡¥æ¥å™¨
                    self.start_stream_bridge()
            except Exception as e:
                logger.warning(f"âš ï¸ æ£€æŸ¥æ¡¥æ¥å™¨çŠ¶æ€å¤±è´¥: {e}ï¼Œå°è¯•é‡æ–°åˆå§‹åŒ–")
                self.start_stream_bridge()
    
    def disconnect(self, websocket: WebSocket, client_type: str):
        """å¤„ç†WebSocketè¿æ¥æ–­å¼€"""
        if websocket in self.active_connections.get(client_type, []):
            self.active_connections[client_type].remove(websocket)
            logger.info(f"{client_type} å®¢æˆ·ç«¯å·²æ–­å¼€ï¼Œå½“å‰è¿æ¥æ•°: {len(self.active_connections[client_type])}")
    
    def start_queue_processor(self):
        """å¯åŠ¨é˜Ÿåˆ—å¤„ç†å™¨"""
        try:
            # æ£€æŸ¥æ˜¯å¦å·²æœ‰è¿è¡Œä¸­çš„ä»»åŠ¡
            if self.processing_task is None or self.processing_task.done():
                try:
                    # å°è¯•è·å–å½“å‰äº‹ä»¶å¾ªç¯
                    loop = asyncio.get_running_loop()
                    self.processing_task = loop.create_task(self.process_queue())
                    logger.info("é˜Ÿåˆ—å¤„ç†å™¨å·²å¯åŠ¨")
                except RuntimeError:
                    # æ²¡æœ‰è¿è¡Œä¸­çš„äº‹ä»¶å¾ªç¯ï¼Œä»»åŠ¡å°†åœ¨WebSocketè¿æ¥æ—¶å¯åŠ¨
                    logger.info("é˜Ÿåˆ—å¤„ç†å™¨å°†åœ¨WebSocketè¿æ¥æ—¶å¯åŠ¨")
                    self.processing_task = None
        except Exception as e:
            logger.error(f"å¯åŠ¨é˜Ÿåˆ—å¤„ç†å™¨å¤±è´¥: {e}")
    
    def adjust_processing_rate(self):
        """åŠ¨æ€è°ƒæ•´å¤„ç†é€Ÿç‡"""
        now = time.time()
        
        # æ¯éš”ä¸€æ®µæ—¶é—´è°ƒæ•´ä¸€æ¬¡
        if now - self.last_rate_adjustment < self.rate_adjustment_interval:
            return
            
        self.last_rate_adjustment = now
        
        # è®¡ç®—å¹³å‡å¤„ç†æ—¶é—´
        if not self.processing_times:
            return
            
        avg_processing_time = sum(self.processing_times) / len(self.processing_times)
        queue_size = len(self.data_queue)
        
        # æ ¹æ®é˜Ÿåˆ—é•¿åº¦å’Œå¤„ç†æ—¶é—´è°ƒæ•´é€Ÿç‡
        if queue_size > 50 and avg_processing_time < 0.05:
            # é˜Ÿåˆ—è¾ƒé•¿ä¸”å¤„ç†é€Ÿåº¦å¿«ï¼Œå¢åŠ å¤„ç†é€Ÿç‡
            self.target_processing_rate = min(self.target_processing_rate + 2, self.max_processing_rate)
            logger.info(f"é˜Ÿåˆ—è¾ƒé•¿({queue_size})ï¼Œå¢åŠ å¤„ç†é€Ÿç‡è‡³: {self.target_processing_rate}/ç§’")
        elif queue_size < 10 and avg_processing_time > 0.1:
            # é˜Ÿåˆ—è¾ƒçŸ­ä¸”å¤„ç†é€Ÿåº¦æ…¢ï¼Œé™ä½å¤„ç†é€Ÿç‡
            self.target_processing_rate = max(self.target_processing_rate - 1, self.min_processing_rate)
            logger.info(f"å¤„ç†è¾ƒæ…¢({avg_processing_time:.3f}s)ï¼Œé™ä½å¤„ç†é€Ÿç‡è‡³: {self.target_processing_rate}/ç§’")
        elif queue_size > 80:
            # é˜Ÿåˆ—æ¥è¿‘æ»¡ï¼Œå¤§å¹…å¢åŠ å¤„ç†é€Ÿç‡
            self.target_processing_rate = self.max_processing_rate
            logger.warning(f"é˜Ÿåˆ—æ¥è¿‘æ»¡({queue_size}/{self.data_queue.maxlen})ï¼Œå¤„ç†é€Ÿç‡è®¾ä¸ºæœ€å¤§: {self.target_processing_rate}/ç§’")
    
    async def process_queue(self):
        """å¤„ç†é˜Ÿåˆ—ä¸­çš„æ•°æ®"""
        logger.info("å¼€å§‹å¤„ç†æ•°æ®é˜Ÿåˆ—")
        try:
            while True:
                # æ£€æŸ¥é˜Ÿåˆ—æ˜¯å¦ä¸ºç©º
                if not self.data_queue:
                    # é˜Ÿåˆ—ä¸ºç©ºï¼Œç­‰å¾…ä¸€æ®µæ—¶é—´
                    await asyncio.sleep(0.1)
                    continue
                
                # å®ç°é€Ÿç‡é™åˆ¶
                current_time = time.time()
                elapsed = current_time - self.last_process_time
                
                if elapsed >= 1.0:  # æ¯ç§’é‡ç½®è®¡æ•°
                    self.last_process_time = current_time
                    self.processed_count = 0
                    # è°ƒæ•´å¤„ç†é€Ÿç‡
                    self.adjust_processing_rate()
                
                if self.processed_count >= self.target_processing_rate:
                    # è¾¾åˆ°æ¯ç§’å¤„ç†ä¸Šé™ï¼Œç­‰å¾…åˆ°ä¸‹ä¸€ç§’
                    await asyncio.sleep(max(0, 1.0 - elapsed))
                    continue
                
                # ä»é˜Ÿåˆ—ä¸­è·å–æ•°æ®
                if self.data_queue:
                    data = self.data_queue.popleft()
                    
                    # è®°å½•å¤„ç†å¼€å§‹æ—¶é—´
                    start_time = time.time()
                    
                    # å¤„ç†æ•°æ®
                    await self.process_data(data)
                    
                    # è®°å½•å¤„ç†æ—¶é—´
                    processing_time = time.time() - start_time
                    self.processing_times.append(processing_time)
                    
                    self.processed_count += 1
                    
                    # è®°å½•é˜Ÿåˆ—çŠ¶æ€
                    queue_size = len(self.data_queue)
                    if queue_size > 0 and queue_size % 10 == 0:
                        logger.info(f"å½“å‰é˜Ÿåˆ—ä¸­è¿˜æœ‰ {queue_size} æ¡æ•°æ®ç­‰å¾…å¤„ç†ï¼Œå¤„ç†é€Ÿç‡: {self.target_processing_rate}/ç§’")
                    
                    # é€‚å½“å»¶è¿Ÿï¼Œé¿å…CPUå ç”¨è¿‡é«˜
                    # æ ¹æ®å¤„ç†æ—¶é—´åŠ¨æ€è°ƒæ•´å»¶è¿Ÿ
                    delay = max(0.01, min(0.05, 0.05 - processing_time))
                    await asyncio.sleep(delay)
        except asyncio.CancelledError:
            logger.info("é˜Ÿåˆ—å¤„ç†å™¨å·²å–æ¶ˆ")
        except Exception as e:
            logger.error(f"é˜Ÿåˆ—å¤„ç†å™¨å‡ºé”™: {e}", exc_info=True)
            # é‡å¯å¤„ç†å™¨
            asyncio.create_task(self.restart_processor())
    
    async def restart_processor(self):
        """é‡å¯é˜Ÿåˆ—å¤„ç†å™¨"""
        await asyncio.sleep(1)  # ç­‰å¾…1ç§’åé‡å¯
        self.start_queue_processor()
        logger.info("é˜Ÿåˆ—å¤„ç†å™¨å·²é‡å¯")
    
    def start_status_monitor(self):
        """å¯åŠ¨çŠ¶æ€ç›‘æ§ä»»åŠ¡"""
        try:
            # æ£€æŸ¥æ˜¯å¦å·²æœ‰è¿è¡Œä¸­çš„ä»»åŠ¡
            if self.status_monitor_task is None or self.status_monitor_task.done():
                try:
                    # å°è¯•è·å–å½“å‰äº‹ä»¶å¾ªç¯
                    loop = asyncio.get_running_loop()
                    self.status_monitor_task = loop.create_task(self.monitor_status())
                    logger.info("çŠ¶æ€ç›‘æ§ä»»åŠ¡å·²å¯åŠ¨")
                except RuntimeError:
                    # æ²¡æœ‰è¿è¡Œä¸­çš„äº‹ä»¶å¾ªç¯ï¼Œä»»åŠ¡å°†åœ¨WebSocketè¿æ¥æ—¶å¯åŠ¨
                    logger.info("çŠ¶æ€ç›‘æ§ä»»åŠ¡å°†åœ¨WebSocketè¿æ¥æ—¶å¯åŠ¨")
                    self.status_monitor_task = None
        except Exception as e:
            logger.error(f"å¯åŠ¨çŠ¶æ€ç›‘æ§ä»»åŠ¡å¤±è´¥: {e}")
    
    async def monitor_status(self):
        """ç›‘æ§ç³»ç»ŸçŠ¶æ€å¹¶æ›´æ–°ç»Ÿè®¡ä¿¡æ¯"""
        try:
            while True:
                # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
                self.update_stats()
                
                # æ¯30ç§’è®°å½•ä¸€æ¬¡ç³»ç»ŸçŠ¶æ€
                if self.stats["processed_count"] > 0 and self.stats["processed_count"] % 100 == 0:
                    self.log_system_status()
                
                # æ¯10ç§’æ£€æŸ¥ä¸€æ¬¡
                await asyncio.sleep(10)
        except asyncio.CancelledError:
            logger.info("çŠ¶æ€ç›‘æ§ä»»åŠ¡å·²å–æ¶ˆ")
        except Exception as e:
            logger.error(f"çŠ¶æ€ç›‘æ§ä»»åŠ¡å‡ºé”™: {e}", exc_info=True)
            # é‡å¯ç›‘æ§ä»»åŠ¡
            asyncio.create_task(self.restart_monitor())
    
    async def restart_monitor(self):
        """é‡å¯çŠ¶æ€ç›‘æ§ä»»åŠ¡"""
        await asyncio.sleep(1)  # ç­‰å¾…1ç§’åé‡å¯
        self.start_status_monitor()
        logger.info("çŠ¶æ€ç›‘æ§ä»»åŠ¡å·²é‡å¯")
    
    def update_stats(self):
        """æ›´æ–°ç»Ÿè®¡ä¿¡æ¯"""
        # æ›´æ–°é˜Ÿåˆ—æœ€å¤§é•¿åº¦
        current_queue_len = len(self.data_queue)
        self.stats["queue_max_length"] = max(self.stats["queue_max_length"], current_queue_len)
        
        # æ›´æ–°å¹³å‡å¤„ç†æ—¶é—´
        if self.processing_times:
            self.stats["avg_processing_time"] = sum(self.processing_times) / len(self.processing_times)
            self.stats["max_processing_time"] = max(self.processing_times)
        
        # æ›´æ–°å½“å‰é‡‡æ ·ç‡å’Œå¤„ç†é€Ÿç‡
        self.stats["current_sampling_rate"] = self.sampling_rate
        self.stats["current_processing_rate"] = self.target_processing_rate
    
    def log_system_status(self):
        """è®°å½•ç³»ç»ŸçŠ¶æ€"""
        uptime = time.time() - self.stats["start_time"]
        hours, remainder = divmod(uptime, 3600)
        minutes, seconds = divmod(remainder, 60)
        uptime_str = f"{int(hours)}h {int(minutes)}m {int(seconds)}s"
        
        # è®¡ç®—å¤„ç†é€Ÿç‡
        processing_rate = self.stats["processed_count"] / uptime if uptime > 0 else 0
        
        # è®°å½•çŠ¶æ€æ—¥å¿—
        logger.info(
            f"ç³»ç»ŸçŠ¶æ€ - è¿è¡Œæ—¶é—´: {uptime_str}, "
            f"é˜Ÿåˆ—: {len(self.data_queue)}/{self.data_queue.maxlen}, "
            f"é‡‡æ ·ç‡: {self.sampling_rate:.2f}, "
            f"å¤„ç†é€Ÿç‡: {self.target_processing_rate}/ç§’, "
            f"å¹³å‡å¤„ç†æ—¶é—´: {self.stats['avg_processing_time']*1000:.2f}ms, "
            f"å·²æ¥æ”¶: {self.stats['received_count']}, "
            f"å·²å¤„ç†: {self.stats['processed_count']}, "
            f"å·²ä¸¢å¼ƒ: {self.stats['dropped_count']}, "
            f"å¹³å‡é€Ÿç‡: {processing_rate:.2f}/ç§’"
        )
    
    async def process_data(self, data: dict):
        """å¤„ç†å•æ¡æ•°æ® - æ”¯æŒå¤šæ•…éšœæ¨¡å¼"""
        try:
            # æ£€æŸ¥æ•°æ®ç±»å‹
            fault_type = data.get("fault_type", "unknown")
            
            # å¦‚æœæ˜¯å¤šæ•…éšœæ•°æ®ï¼Œéœ€è¦åˆ†åˆ«å¤„ç†æ¯ç§æ•…éšœç±»å‹
            if fault_type == "multi_fault":
                await self.process_multi_fault_data(data)
                return
            
            # å¤„ç†å•ä¸€æ•…éšœç±»å‹
            await self.process_single_fault_data(data, fault_type)
            
        except Exception as e:
            logger.error(f"å¤„ç†æ•°æ®æ—¶å‡ºé”™: {e}", exc_info=True)
            self.stats["error_count"] += 1

    async def process_multi_fault_data(self, data: dict):
        """å¤„ç†å¤šæ•…éšœæ··åˆæ•°æ®"""
        try:
            active_faults = data.get("active_faults", [])
            fault_severities = data.get("fault_severities", {})
            
            if not active_faults:
                # å¦‚æœæ²¡æœ‰æ´»è·ƒæ•…éšœï¼Œä»ç„¶å‘é€åŸºç¡€æ•°æ®ç»™å‰ç«¯
                await self.send_basic_multi_fault_result(data)
                return
            
            # æ”¶é›†æ¯ä¸ªæ•…éšœç±»å‹çš„åˆ†æç‰¹å¾
            fault_features = {}
            
            # ä¸ºæ¯ä¸ªæ´»è·ƒçš„æ•…éšœç±»å‹ç”Ÿæˆåˆ†æç»“æœ
            for fault_type in active_faults:
                if fault_type in self.analyzers:
                    # åˆ›å»ºå•æ•…éšœæ•°æ®æ ¼å¼ä¾›åˆ†æå™¨ä½¿ç”¨
                    single_fault_data = self.extract_single_fault_data(data, fault_type)
                    
                    # è¿è¡Œåˆ†æå™¨è·å–ç‰¹å¾
                    analyzer = self.analyzers[fault_type]
                    analysis_result = analyzer.analyze(single_fault_data)
                    
                    if analysis_result and 'features' in analysis_result:
                        fault_features[fault_type] = analysis_result['features']
                        
                        # åŒæ—¶å‘é€å•ç‹¬çš„æ•…éšœåˆ†æç»“æœ
                        result = {
                            **analysis_result,
                            "fault_type": fault_type,
                            "data_source": data.get("connection_id", "unknown"),
                            "timestamp": datetime.now().isoformat(),
                            "queue_size": len(self.data_queue),
                            "system_stats": {
                                "sampling_rate": self.sampling_rate,
                                "processing_rate": self.target_processing_rate,
                                "queue_usage": len(self.data_queue) / self.data_queue.maxlen if self.data_queue.maxlen else 0
                            }
                        }
                        
                        # æ·»åŠ è¯¦ç»†è°ƒè¯•æ—¥å¿—
                        logger.info(f"å‘é€ {fault_type} åˆ†æç»“æœ - è¯„åˆ†: {result.get('score', 0):.3f}, çŠ¶æ€: {result.get('status', 'unknown')}")
                        logger.info(f"{fault_type} ç‰¹å¾æ•°æ®: {result.get('features', {})}")
                        
                        # æ£€æŸ¥é¢‘è°±æ•°æ®
                        if 'frequency_spectrum' in result:
                            freq_data = result['frequency_spectrum']
                            freq_len = len(freq_data.get('frequency', []))
                            logger.info(f"{fault_type} é¢‘è°±æ•°æ®å·²åŒ…å«ï¼Œé¢‘ç‡ç‚¹æ•°: {freq_len}")
                            if freq_len == 0:
                                logger.warning(f"{fault_type} é¢‘è°±æ•°æ®ä¸ºç©ºï¼")
                        else:
                            logger.warning(f"{fault_type} åˆ†æç»“æœä¸­ç¼ºå°‘é¢‘è°±æ•°æ®ï¼")
                        
                        # æ£€æŸ¥æ—¶é—´åºåˆ—æ•°æ®
                        if 'time_series' in result:
                            logger.info(f"{fault_type} åŒ…å«æ—¶é—´åºåˆ—æ•°æ®")
                        else:
                            logger.info(f"{fault_type} ä¸åŒ…å«æ—¶é—´åºåˆ—æ•°æ®")
                        
                        await self.broadcast_to_frontends(result)
            
            # å‘é€ç»¼åˆçš„å¤šæ•…éšœç»“æœï¼ŒåŒ…å«æ‰€æœ‰ç‰¹å¾
            await self.send_comprehensive_multi_fault_result(data, active_faults, fault_severities, fault_features)
            
        except Exception as e:
            logger.error(f"å¤„ç†å¤šæ•…éšœæ•°æ®æ—¶å‡ºé”™: {e}")
            self.stats["error_count"] += 1

    def extract_single_fault_data(self, multi_fault_data: dict, fault_type: str) -> dict:
        """ä»å¤šæ•…éšœæ•°æ®ä¸­æå–å•æ•…éšœæ•°æ®æ ¼å¼"""
        # å¤åˆ¶åŸºç¡€æ•°æ®ç»“æ„
        single_data = {
            "timestamp": multi_fault_data.get("timestamp"),
            "connection_id": multi_fault_data.get("connection_id", "unknown"),
            "fault_type": fault_type,
            "fault_active": True,
            "fault_severity": multi_fault_data.get("fault_severities", {}).get(fault_type, 0.0),
            "data": multi_fault_data.get("data", []),
            "sampling_rate": multi_fault_data.get("sampling_rate", 8000),
            "batch_size": multi_fault_data.get("batch_size", 0)
        }
        return single_data

    async def send_basic_multi_fault_result(self, data: dict):
        """å‘é€åŸºç¡€å¤šæ•…éšœç»“æœï¼ˆæ— æ´»è·ƒæ•…éšœæ—¶ï¼‰"""
        basic_result = {
            "fault_type": "multi_fault",
            "data_source": data.get("connection_id", "unknown"),
            "timestamp": datetime.now().isoformat(),
            "status": "normal",
            "score": 0.0,
            "active_faults": [],
            "fault_severities": data.get("fault_severities", {}),
            "system_stats": {
                "sampling_rate": self.sampling_rate,
                "processing_rate": self.target_processing_rate,
                "queue_usage": len(self.data_queue) / self.data_queue.maxlen if self.data_queue.maxlen else 0
            }
        }
        
        # æ·»åŠ æ—¶é—´åºåˆ—æ•°æ®
        if 'data' in data and data['data']:
            df = pd.DataFrame(data['data'])
            if 'æ—¶é—´' in df.columns:
                sample_rate = max(1, len(df) // 200)
                time_series = {"time": df['æ—¶é—´'].iloc[::sample_rate].tolist()}
                
                # æ·»åŠ ä¸»è¦ä¿¡å·
                if all(col in df.columns for col in ['Ia', 'Ib', 'Ic']):
                    time_series["values_a"] = df['Ia'].iloc[::sample_rate].tolist()
                    time_series["values_b"] = df['Ib'].iloc[::sample_rate].tolist()
                    time_series["values_c"] = df['Ic'].iloc[::sample_rate].tolist()
                
                basic_result["time_series"] = time_series
        
        await self.broadcast_to_frontends(basic_result)

    async def send_comprehensive_multi_fault_result(self, data: dict, active_faults: list, fault_severities: dict, fault_features: dict):
        """å‘é€ç»¼åˆå¤šæ•…éšœåˆ†æç»“æœ"""
        # è®¡ç®—ç»¼åˆè¯„åˆ†
        max_severity = max(fault_severities.values()) if fault_severities else 0.0
        avg_severity = sum(fault_severities.values()) / len(fault_severities) if fault_severities else 0.0
        
        # ç¡®å®šæ•´ä½“çŠ¶æ€
        if max_severity > 0.7:
            overall_status = "critical"
        elif max_severity > 0.5:
            overall_status = "warning"
        elif max_severity > 0.2:
            overall_status = "attention"
        else:
            overall_status = "normal"
        
        comprehensive_result = {
            "fault_type": "multi_fault",
            "data_source": data.get("connection_id", "unknown"),
            "timestamp": datetime.now().isoformat(),
            "status": overall_status,
            "score": max_severity,
            "avg_score": avg_severity,
            "active_faults": active_faults,
            "fault_severities": fault_severities,
            "fault_count": len(active_faults),
            "fault_features": fault_features, # æ·»åŠ æ•…éšœç‰¹å¾
            "system_stats": {
                "sampling_rate": self.sampling_rate,
                "processing_rate": self.target_processing_rate,
                "queue_usage": len(self.data_queue) / self.data_queue.maxlen if self.data_queue.maxlen else 0
            }
        }
        
        # å¦‚æœåŒ…å«åŒé—´çŸ­è·¯æ•…éšœï¼Œæ·»åŠ é¢‘è°±æ•°æ®
        if 'turn_fault' in active_faults and 'turn_fault' in self.analyzers:
            try:
                # é‡æ–°è¿è¡ŒåŒé—´çŸ­è·¯åˆ†æå™¨æ¥è·å–é¢‘è°±æ•°æ®
                single_fault_data = self.extract_single_fault_data(data, 'turn_fault')
                turn_result = self.analyzers['turn_fault'].analyze(single_fault_data)
                if turn_result and 'frequency_spectrum' in turn_result:
                    comprehensive_result['frequency_spectrum'] = turn_result['frequency_spectrum']
                    logger.info(f"æ·»åŠ åŒé—´çŸ­è·¯é¢‘è°±æ•°æ®åˆ°ç»¼åˆç»“æœï¼Œé¢‘ç‡ç‚¹æ•°: {len(turn_result['frequency_spectrum'].get('frequency', []))}")
            except Exception as e:
                logger.error(f"è·å–åŒé—´çŸ­è·¯é¢‘è°±æ•°æ®æ—¶å‡ºé”™: {e}")
        
        # æ·»åŠ è¯¦ç»†çš„æ—¶é—´åºåˆ—æ•°æ®
        if 'data' in data and data['data']:
            df = pd.DataFrame(data['data'])
            if 'æ—¶é—´' in df.columns:
                sample_rate = max(1, len(df) // 200)
                time_series = {"time": df['æ—¶é—´'].iloc[::sample_rate].tolist()}
                
                # æ·»åŠ æ‰€æœ‰å¯ç”¨çš„ä¿¡å·
                if all(col in df.columns for col in ['Ia', 'Ib', 'Ic']):
                    time_series["values_a"] = df['Ia'].iloc[::sample_rate].tolist()
                    time_series["values_b"] = df['Ib'].iloc[::sample_rate].tolist()
                    time_series["values_c"] = df['Ic'].iloc[::sample_rate].tolist()
                
                # æ·»åŠ æŒ¯åŠ¨ä¿¡å·
                if 'vibration_x' in df.columns:
                    time_series["vibration_x"] = df['vibration_x'].iloc[::sample_rate].tolist()
                if 'vibration_y' in df.columns:
                    time_series["vibration_y"] = df['vibration_y'].iloc[::sample_rate].tolist()
                
                # æ·»åŠ ç»ç¼˜ç‰¹å¾
                if 'insulation_resistance' in df.columns:
                    time_series["insulation_resistance"] = df['insulation_resistance'].iloc[::sample_rate].tolist()
                if 'leakage_current' in df.columns:
                    time_series["leakage_current"] = df['leakage_current'].iloc[::sample_rate].tolist()
                
                # æ·»åŠ æ¸©åº¦
                if 'temperature' in df.columns:
                    time_series["temperature"] = df['temperature'].iloc[::sample_rate].tolist()
                
                comprehensive_result["time_series"] = time_series
        
        await self.broadcast_to_frontends(comprehensive_result)

    async def process_single_fault_data(self, data: dict, fault_type: str):
        """å¤„ç†å•ä¸€æ•…éšœæ•°æ®"""
        # é€‰æ‹©å¯¹åº”çš„åˆ†æå™¨
        analyzer = self.analyzers.get(fault_type)
        if not analyzer:
            logger.warning(f"æœªæ‰¾åˆ°æ•…éšœç±»å‹ '{fault_type}' çš„åˆ†æå™¨")
            return
        
        # åˆ†ææ•°æ®
        result = analyzer.analyze(data)
        
        if result:
            # æ·»åŠ æ ‡å‡†å­—æ®µï¼Œç¬¦åˆæ•°æ®æµé€»è¾‘
            result.update({
                "fault_type": fault_type,
                "vehicle_id": data.get("vehicle_id", data.get("connection_id", "unknown")),
                "timestamp": datetime.now().isoformat(),
                "location": data.get("location", self._extract_location_from_vehicle_id(data.get("vehicle_id", ""))),
            })
            
            # å¹¿æ’­åˆ°å‰ç«¯
            await self.broadcast_to_frontends(result)
    
    def _extract_location_from_vehicle_id(self, vehicle_id: str) -> str:
        """ä»è½¦è¾†IDæå–ä½ç½®ä¿¡æ¯"""
        if not vehicle_id:
            return "æœªçŸ¥ä½ç½®"
        
        # åœ°åŒºæ˜ å°„
        location_map = {
            "ç²¤B": "æ·±åœ³ç¦ç”°åŒº",
            "é™•A": "è¥¿å®‰é«˜æ–°åŒº", 
            "ç²¤A": "å¹¿å·å¤©æ²³åŒº",
            "æ²ªA": "ä¸Šæµ·æµ¦ä¸œåŒº",
            "äº¬A": "åŒ—äº¬æµ·æ·€åŒº",
            "è‹A": "å—äº¬é¼“æ¥¼åŒº",
            "æµ™A": "æ­å·æ»¨æ±ŸåŒº",
            "å·A": "æˆéƒ½é«˜æ–°åŒº",
            "æ¸A": "é‡åº†æ¸ä¸­åŒº"
        }
        
        for region, location in location_map.items():
            if region in vehicle_id:
                return location
        
        return "æœªçŸ¥ä½ç½®"

    def enqueue_data(self, data: dict) -> bool:
        """å°†æ•°æ®åŠ å…¥å¤„ç†é˜Ÿåˆ—ï¼Œå®ç°èƒŒå‹æ§åˆ¶"""
        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        self.stats["received_count"] += 1
        
        # å¦‚æœå¯ç”¨äº†Redisï¼Œå‘é€åŸå§‹æ•°æ®åˆ°Redis
        if self.use_redis:
            # å¼‚æ­¥å‘é€åˆ°Redisï¼Œä½†ä¸ç­‰å¾…ç»“æœ
            asyncio.create_task(self._send_to_redis(data))
        
        # æ£€æŸ¥é˜Ÿåˆ—æ˜¯å¦å·²æ»¡ï¼ˆself.data_queue.maxlen å¯èƒ½ä¸º Noneï¼Œéœ€è¦å®‰å…¨æ¯”è¾ƒï¼‰
        queue_maxlen = self.data_queue.maxlen if self.data_queue.maxlen is not None else 0
        if queue_maxlen > 0 and len(self.data_queue) >= queue_maxlen:
            if self.use_redis:
                # å¦‚æœRediså¯ç”¨ï¼Œä¼˜å…ˆä½¿ç”¨Rediså¤„ç†ï¼Œè·³è¿‡æœ¬åœ°é˜Ÿåˆ—
                logger.info(f"æœ¬åœ°é˜Ÿåˆ—å·²æ»¡ ({queue_maxlen})ï¼Œæ•°æ®å·²å‘é€è‡³Redisé˜Ÿåˆ—å¤„ç†")
                return True  # è¿”å›æˆåŠŸï¼Œå› ä¸ºæ•°æ®å·²å‘é€åˆ°Redis
            else:
                # Redisä¸å¯ç”¨æ—¶ï¼Œåªèƒ½ä¸¢å¼ƒæ•°æ®
                logger.warning(f"æ•°æ®é˜Ÿåˆ—å·²æ»¡ ({queue_maxlen})ï¼Œä¸¢å¼ƒæ•°æ®")
                # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
                self.stats["dropped_count"] += 1
                # è°ƒæ•´é‡‡æ ·ç‡
                self.adjust_sampling_rate()
                return False
        
        # é‡‡æ ·æ§åˆ¶
        if not self.should_sample_data(data):
            self.stats["sampled_count"] += 1
            return True  # è¿”å›æˆåŠŸä½†å®é™…ä¸å¤„ç†
        
        # åŠ å…¥é˜Ÿåˆ—
        self.data_queue.append(data)
        
        # å®šæœŸè°ƒæ•´é‡‡æ ·ç‡
        if random.random() < 0.05:  # 5%çš„æ¦‚ç‡è°ƒæ•´é‡‡æ ·ç‡
            self.adjust_sampling_rate()
            
        return True
    
    async def _send_to_redis(self, data: dict):
        """å‘é€æ•°æ®åˆ°Redis"""
        try:
            success = await simple_queue.send_message(TOPICS['FAULT_DATA'], data)
            if success:
                self.stats["redis_sent_count"] += 1
                logger.debug(f"æ•°æ®å·²å‘é€åˆ°Redis: {data.get('connection_id', 'unknown')}")
            else:
                self.stats["redis_error_count"] += 1
                logger.warning(f"å‘é€æ•°æ®åˆ°Rediså¤±è´¥")
        except Exception as e:
            logger.error(f"å‘é€æ•°æ®åˆ°Redisæ—¶å‡ºé”™: {e}")
            self.stats["redis_error_count"] += 1
    
    async def handle_redis_result(self, result: dict):
        """å¤„ç†ä»Redisæ¥æ”¶çš„åˆ†æç»“æœ"""
        try:
            # å¹¿æ’­ç»“æœåˆ°å‰ç«¯
            await self.broadcast_to_frontends(result)
            logger.debug(f"ä»Redisæ¥æ”¶çš„åˆ†æç»“æœå·²å¹¿æ’­åˆ°å‰ç«¯")
        except Exception as e:
            logger.error(f"å¤„ç†Redisåˆ†æç»“æœæ—¶å‡ºé”™: {e}")
    
    def get_system_stats(self) -> dict:
        """è·å–ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯ï¼Œç”¨äºå¿ƒè·³å“åº”"""
        self.update_stats()
        stats = {
            "uptime": time.time() - self.stats["start_time"],
            "queue_size": len(self.data_queue),
            "queue_capacity": self.data_queue.maxlen,
            "queue_usage": len(self.data_queue) / self.data_queue.maxlen if self.data_queue.maxlen else 0,
            "sampling_rate": self.sampling_rate,
            "processing_rate": self.target_processing_rate,
            "avg_processing_time": self.stats["avg_processing_time"],
            "received_count": self.stats["received_count"],
            "processed_count": self.stats["processed_count"],
            "dropped_count": self.stats["dropped_count"],
            "error_count": self.stats["error_count"]
        }
        
        # æ·»åŠ Redisç›¸å…³ç»Ÿè®¡
        if self.use_redis:
            stats.update({
                "redis_enabled": True,
                "redis_sent_count": self.stats["redis_sent_count"],
                "redis_error_count": self.stats["redis_error_count"]
            })
        else:
            stats["redis_enabled"] = False
            
        return stats
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        if self.processing_task:
            self.processing_task.cancel()
        if self.status_monitor_task:
            self.status_monitor_task.cancel()
    
    def should_sample_data(self, data: dict) -> bool:
        """å†³å®šæ˜¯å¦å¯¹æ•°æ®è¿›è¡Œé‡‡æ ·å¤„ç†"""
        if not self.sampling_enabled:
            return True
            
        # è·å–æ•°æ®æºID
        source_id = data.get("connection_id", "unknown")
        
        # æ£€æŸ¥æ•°æ®æºçš„ä¸Šæ¬¡å¤„ç†æ—¶é—´
        now = time.time()
        last_time = self.last_data_timestamp.get(source_id, 0)
        if now - last_time < self.min_time_between_samples:
            # æ•°æ®å¤ªé¢‘ç¹ï¼Œéœ€è¦é‡‡æ ·
            self.data_source_counter[source_id] = self.data_source_counter.get(source_id, 0) + 1
            
            # æ ¹æ®é‡‡æ ·ç‡å†³å®šæ˜¯å¦å¤„ç†
            if random.random() > self.sampling_rate:
                return False
                
        # æ›´æ–°æœ€åå¤„ç†æ—¶é—´
        self.last_data_timestamp[source_id] = now
        return True
    
    def adjust_sampling_rate(self):
        """æ ¹æ®é˜Ÿåˆ—é•¿åº¦è°ƒæ•´é‡‡æ ·ç‡"""
        queue_size = len(self.data_queue)
        queue_capacity = self.data_queue.maxlen
        
        # é˜Ÿåˆ—ä½¿ç”¨ç‡
        usage_ratio = queue_size / queue_capacity if queue_capacity else 0
        
        # æ ¹æ®é˜Ÿåˆ—ä½¿ç”¨ç‡è°ƒæ•´é‡‡æ ·ç‡
        if usage_ratio > 0.8:  # é˜Ÿåˆ—ä½¿ç”¨è¶…è¿‡80%
            self.sampling_rate = max(0.1, self.sampling_rate - 0.2)
            logger.warning(f"é˜Ÿåˆ—æ¥è¿‘æ»¡è½½ ({queue_size}/{queue_capacity})ï¼Œé™ä½é‡‡æ ·ç‡è‡³ {self.sampling_rate:.1f}")
        elif usage_ratio > 0.5:  # é˜Ÿåˆ—ä½¿ç”¨è¶…è¿‡50%
            self.sampling_rate = max(0.3, self.sampling_rate - 0.1)
            logger.info(f"é˜Ÿåˆ—è´Ÿè½½è¾ƒé«˜ ({queue_size}/{queue_capacity})ï¼Œé™ä½é‡‡æ ·ç‡è‡³ {self.sampling_rate:.1f}")
        elif usage_ratio < 0.2 and self.sampling_rate < 1.0:  # é˜Ÿåˆ—ä½¿ç”¨ä½äº20%
            self.sampling_rate = min(1.0, self.sampling_rate + 0.1)
            logger.info(f"é˜Ÿåˆ—è´Ÿè½½è¾ƒä½ ({queue_size}/{queue_capacity})ï¼Œæé«˜é‡‡æ ·ç‡è‡³ {self.sampling_rate:.1f}")
    
    async def broadcast_to_frontends(self, message: dict):
        """å¹¿æ’­æ¶ˆæ¯åˆ°æ‰€æœ‰å‰ç«¯è¿æ¥"""
        if not self.active_connections["frontend"]:
            return
        
        # æ ‡å‡†åŒ–æ¶ˆæ¯æ ¼å¼ï¼Œç¬¦åˆæ•°æ®æµé€»è¾‘
        standardized_message = self._standardize_frontend_message(message)
        
        # åºåˆ—åŒ–æ¶ˆæ¯
        encoded_message = json.dumps(standardized_message)
        
        # å¹¿æ’­åˆ°æ‰€æœ‰å‰ç«¯
        disconnected = []
        for connection in self.active_connections["frontend"]:
            try:
                await connection.send_text(encoded_message)
            except Exception as e:
                logger.error(f"å¹¿æ’­æ¶ˆæ¯å¤±è´¥: {e}")
                disconnected.append(connection)
        
        # æ¸…ç†æ–­å¼€çš„è¿æ¥
        for conn in disconnected:
            self.disconnect(conn, "frontend")
    
    def _standardize_frontend_message(self, message: dict) -> dict:
        """æ ‡å‡†åŒ–å‰ç«¯æ¶ˆæ¯æ ¼å¼ï¼Œç¬¦åˆæ•°æ®æµé€»è¾‘"""
        # åŸºç¡€æ ‡å‡†æ ¼å¼
        standardized = {
            "fault_type": message.get("fault_type", "unknown"),
            "vehicle_id": message.get("vehicle_id", message.get("data_source", "unknown")),
            "timestamp": message.get("timestamp", datetime.now().isoformat()),
            "status": message.get("status", "unknown"),
            "score": float(message.get("score", 0.0)),  # ç¡®ä¿scoreä¸ºæ•°å€¼ç±»å‹
        }
        
        # æ·»åŠ ç‰¹å¾æ•°æ®
        if "features" in message:
            standardized["features"] = message["features"]
        
        # æ·»åŠ æ—¶é—´åºåˆ—æ•°æ®
        if "time_series" in message:
            standardized["time_series"] = message["time_series"]
        
        # æ·»åŠ é¢‘è°±æ•°æ®
        if "frequency_spectrum" in message:
            standardized["spectrum"] = message["frequency_spectrum"]
        elif "spectrum" in message:
            standardized["spectrum"] = message["spectrum"]
        
        # æ·»åŠ å›¾è¡¨æ•°æ®
        charts = {}
        if "charts" in message:
            charts = message["charts"]
        if "time_domain" in message:
            charts["time_domain"] = message["time_domain"]
        if "frequency_domain" in message:
            charts["frequency_domain"] = message["frequency_domain"]
        if charts:
            standardized["charts"] = charts
        
        # æ·»åŠ ä½ç½®ä¿¡æ¯ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if "location" in message:
            standardized["location"] = message["location"]
        
        # è®¡ç®—å¥åº·è¯„åˆ†
        score = standardized["score"]
        if score <= 0.2:
            health_score = 95.0 - (score * 25)  # æ­£å¸¸èŒƒå›´ 95-90
        elif score <= 0.5:
            health_score = 90.0 - ((score - 0.2) * 100)  # è­¦å‘ŠèŒƒå›´ 90-60
        else:
            health_score = 60.0 - ((score - 0.5) * 120)  # æ•…éšœèŒƒå›´ 60-0
        
        standardized["health_score"] = max(0.0, min(100.0, health_score))
        
        return standardized
    
    def start_stream_bridge(self):
        """å¯åŠ¨Redis Streamæ¡¥æ¥ç»„ä»¶ï¼ˆè½»é‡çº§æ•°æ®è½¬å‘ï¼‰"""
        if not self.bridge_initialized:
            # å†æ¬¡æ£€æŸ¥å…¨å±€æ¡¥æ¥å™¨çŠ¶æ€ï¼Œé¿å…é‡å¤åˆå§‹åŒ–
            try:
                from ..services.redis_stream.stream_to_frontend_bridge import stream_bridge
                if stream_bridge.redis_client is not None and stream_bridge.websocket_manager is not None:
                    self.bridge_initialized = True
                    logger.info("ğŸš€ æ£€æµ‹åˆ°å…¨å±€æ¡¥æ¥å™¨å·²å°±ç»ªï¼Œè·³è¿‡é‡å¤åˆå§‹åŒ–")
                    return
            except Exception as e:
                logger.debug(f"æ£€æŸ¥æ¡¥æ¥å™¨çŠ¶æ€å¼‚å¸¸: {e}")
            
            # éœ€è¦åˆå§‹åŒ–
            asyncio.create_task(self._initialize_stream_bridge())
        else:
            logger.debug("ğŸ”— Redis Streamæ¡¥æ¥ç»„ä»¶å·²åˆå§‹åŒ–ï¼Œè·³è¿‡å¯åŠ¨")
    
    async def _initialize_stream_bridge(self):
        """å¼‚æ­¥åˆå§‹åŒ–Redis Streamæ¡¥æ¥ç»„ä»¶"""
        try:
            # å†æ¬¡æ£€æŸ¥æ˜¯å¦å·²åˆå§‹åŒ–ï¼Œé¿å…å¹¶å‘åˆå§‹åŒ–
            if self.bridge_initialized:
                logger.debug("ğŸš€ æ¡¥æ¥ç»„ä»¶å·²åˆå§‹åŒ–ï¼Œå–æ¶ˆé‡å¤åˆå§‹åŒ–")
                return
            
            logger.info("ğŸ”— åˆå§‹åŒ–Redis Streamåˆ°å‰ç«¯æ¡¥æ¥ç»„ä»¶...")
            
            # åˆå§‹åŒ–æ¡¥æ¥ç»„ä»¶
            success = await stream_bridge.initialize(self)
            
            if success:
                self.bridge_initialized = True
                logger.info("âœ… Redis Streamæ¡¥æ¥ç»„ä»¶åˆå§‹åŒ–æˆåŠŸ")
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦å¯åŠ¨ç›‘å¬ä»»åŠ¡
                if not stream_bridge.is_monitoring:
                    asyncio.create_task(stream_bridge.start_monitoring())
                    logger.info("ğŸš€ Redis Streamæ¡¥æ¥ç›‘å¬å·²å¯åŠ¨")
                else:
                    logger.info("ğŸ“Š Redis Streamæ¡¥æ¥ç›‘å¬å·²åœ¨è¿è¡Œ")
            else:
                logger.error("âŒ Redis Streamæ¡¥æ¥ç»„ä»¶åˆå§‹åŒ–å¤±è´¥")
                
        except Exception as e:
            logger.error(f"âŒ å¯åŠ¨Redis Streamæ¡¥æ¥ç»„ä»¶å¤±è´¥: {e}")
            # ä¸å½±å“ä¸»ç³»ç»Ÿè¿è¡Œï¼Œç»§ç»­è¿è¡Œ

# åˆ›å»ºè¿æ¥ç®¡ç†å™¨å®ä¾‹
manager = ConnectionManager()

# æ·»åŠ å¿ƒè·³å“åº”å¤„ç†
async def handle_heartbeat(websocket: WebSocket, data: dict):
    """å¤„ç†å¿ƒè·³è¯·æ±‚"""
    try:
        # è·å–ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯
        stats = manager.get_system_stats()
        
        response = {
            "type": "heartbeat_response",
            "timestamp": datetime.now().isoformat(),
            "server_time": time.time(),
            "queue_size": len(manager.data_queue),
            "processing_rate": manager.target_processing_rate,
            "sampling_rate": manager.sampling_rate,
            "stats": stats
        }
        await websocket.send_text(json.dumps(response))
    except Exception as e:
        logger.error(f"å‘é€å¿ƒè·³å“åº”å¤±è´¥: {e}")

async def handle_frontend_connection(websocket: WebSocket):
    """å¤„ç†å‰ç«¯WebSocketè¿æ¥"""
    await manager.connect(websocket, "frontend")
    try:
        while True:
            # æ¥æ”¶å‰ç«¯å‘½ä»¤ï¼ˆå¦‚æœæœ‰ï¼‰
            data_str = await websocket.receive_text()
            try:
                data = json.loads(data_str)
                
                # å¤„ç†å¿ƒè·³è¯·æ±‚
                if data.get("type") == "heartbeat":
                    await handle_heartbeat(websocket, data)
                # è¿™é‡Œå¯ä»¥æ·»åŠ å¤„ç†å…¶ä»–å‰ç«¯å‘½ä»¤çš„é€»è¾‘
            except json.JSONDecodeError:
                logger.warning(f"æ”¶åˆ°æ— æ•ˆçš„JSONæ•°æ®: {data_str[:100]}")
            
            await asyncio.sleep(0.1)
    except WebSocketDisconnect:
        manager.disconnect(websocket, "frontend")
    except Exception as e:
        logger.error(f"å¤„ç†å‰ç«¯è¿æ¥æ—¶å‡ºé”™: {e}", exc_info=True)
        manager.disconnect(websocket, "frontend")

async def handle_datasource_connection(websocket: WebSocket):
    """å¤„ç†æ•°æ®æºWebSocketè¿æ¥"""
    await manager.connect(websocket, "datasource")
    try:
        while True:
            # æ¥æ”¶æ•°æ®æºå‘é€çš„æ•°æ®
            data_str = await websocket.receive_text()
            data = json.loads(data_str)
            
            # å°†æ•°æ®åŠ å…¥é˜Ÿåˆ—
            success = manager.enqueue_data(data)
            
            # å‘é€ç¡®è®¤ï¼ŒåŒ…å«é˜Ÿåˆ—çŠ¶æ€ï¼ˆå‡å°‘ç¡®è®¤é¢‘ç‡ä»¥é™ä½ç½‘ç»œè´Ÿè½½ï¼‰
            if success and len(manager.data_queue) % 10 == 0:  # æ¯10ä¸ªæ•°æ®åŒ…å‘é€ä¸€æ¬¡ç¡®è®¤
                response = {
                    "status": "received_batch",
                    "queue_size": len(manager.data_queue),
                    "timestamp": datetime.now().isoformat(),
                    "processed_count": manager.stats["processed_count"]
                }
                try:
                    await websocket.send_text(json.dumps(response))
                except Exception as e:
                    logger.debug(f"å‘é€ç¡®è®¤æ¶ˆæ¯å¤±è´¥: {e}")  # é™çº§ä¸ºdebugæ—¥å¿—
    except WebSocketDisconnect:
        manager.disconnect(websocket, "datasource")
    except Exception as e:
        logger.error(f"å¤„ç†æ•°æ®æºè¿æ¥æ—¶å‡ºé”™: {e}", exc_info=True)
        manager.disconnect(websocket, "datasource") 

# Redisæ¶ˆæ¯å¤„ç†å‡½æ•°
async def handle_fault_data_from_redis(data: dict):
    """å¤„ç†ä»Redisæ¥æ”¶çš„æ•…éšœæ•°æ®"""
    # è¿™é‡Œå¯ä»¥ç›´æ¥å¤„ç†æ•°æ®æˆ–åŠ å…¥é˜Ÿåˆ—
    # åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œæˆ‘ä»¬å¯ä»¥ç®€å•è®°å½•ï¼Œä½†ä¸åšé¢å¤–å¤„ç†
    logger.debug(f"ä»Redisæ¥æ”¶åˆ°æ•…éšœæ•°æ®: {data.get('connection_id', 'unknown')}")

async def handle_analysis_result_from_redis(result: dict):
    """å¤„ç†ä»Redisæ¥æ”¶çš„åˆ†æç»“æœ"""
    await manager.handle_redis_result(result)

# åº”ç”¨å…³é—­æ—¶æ¸…ç†èµ„æº
async def shutdown_event():
    """åº”ç”¨å…³é—­æ—¶æ‰§è¡Œæ¸…ç†"""
    manager.cleanup()
    logger.info("å·²æ¸…ç†WebSocketè¿æ¥ç®¡ç†å™¨èµ„æº") 