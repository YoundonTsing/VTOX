#!/usr/bin/env python3
"""
Redis Streamç¼“å­˜ä¼˜åŒ–ç»„ä»¶
åˆ©ç”¨Redis Streamçš„ç¼“å­˜ç‰¹æ€§ä¼˜åŒ–æ¶ˆæ¯å¤„ç†æ€§èƒ½ï¼Œè§£å†³æ¶ˆæ¯ä¸¢å¤±é—®é¢˜
"""

import asyncio
import json
import logging
from typing import Dict, Any, Optional, List, Set
from datetime import datetime, timedelta
import redis.asyncio as redis
from collections import defaultdict
import time

logger = logging.getLogger(__name__)

class StreamCacheOptimizer:
    """
    Redis Streamç¼“å­˜ä¼˜åŒ–å™¨
    
    æ ¸å¿ƒç‰¹æ€§ï¼š
    1. æ¶ˆæ¯æŒä¹…åŒ–ï¼šåˆ©ç”¨Redis StreamæŒä¹…åŒ–ç‰¹æ€§
    2. æ™ºèƒ½é™é‡‡æ ·ï¼šé«˜é¢‘æ•°æ®æ™ºèƒ½é‡‡æ ·ï¼Œå‡å°‘å‰ç«¯å‹åŠ›
    3. æ‰¹é‡å¤„ç†ï¼šä¼˜åŒ–æ‰¹å¤„ç†ç­–ç•¥ï¼Œæé«˜ååé‡
    4. æ¶ˆæ¯é‡è¯•ï¼šå¤±è´¥æ¶ˆæ¯è‡ªåŠ¨é‡è¯•æœºåˆ¶
    5. ç¼“å­˜ç®¡ç†ï¼šæ™ºèƒ½ç¼“å­˜æ¸…ç†å’Œå†…å­˜ä¼˜åŒ–
    """
    
    def __init__(self, redis_url: str = "redis://localhost:6379"):
        self.redis_url = redis_url
        self.redis_client: Optional[redis.Redis] = None
        self.websocket_manager = None
        self.is_running = False
        self._initialized = False  # æ·»åŠ åˆå§‹åŒ–çŠ¶æ€æ ‡å¿—
        
        # ğŸš€ é«˜æ€§èƒ½é…ç½®
        self.max_batch_size = 100          # å¢åŠ æ‰¹å¤„ç†å¤§å°
        self.processing_interval = 0.05    # 50mså¤„ç†é—´éš”ï¼Œ20fps
        self.cache_ttl = 3600             # ç¼“å­˜TTL 1å°æ—¶
        self.max_pending_messages = 10000  # æœ€å¤§å¾…å¤„ç†æ¶ˆæ¯æ•°
        
        # ğŸ§  æ™ºèƒ½é™é‡‡æ ·é…ç½®
        self.sampling_rates = {
            "high_frequency": 0.1,    # é«˜é¢‘æ•°æ®10%é‡‡æ ·ç‡
            "medium_frequency": 0.3,  # ä¸­é¢‘æ•°æ®30%é‡‡æ ·ç‡
            "low_frequency": 1.0,     # ä½é¢‘æ•°æ®100%ä¿ç•™
            "critical_alerts": 1.0    # å…³é”®è­¦æŠ¥100%ä¿ç•™
        }
        
        # ğŸ“Š æ€§èƒ½ç»Ÿè®¡
        self.stats = {
            "total_received": 0,
            "total_processed": 0,
            "total_cached": 0,
            "total_sampled": 0,
            "cache_hits": 0,
            "cache_misses": 0,
            "retry_count": 0,
            "error_count": 0,
            "start_time": None
        }
        
        # ğŸ”„ æ¶ˆæ¯å¤„ç†é˜Ÿåˆ—
        self.processing_queue = asyncio.Queue(maxsize=self.max_pending_messages)
        self.retry_queue = asyncio.Queue(maxsize=1000)
        
        # ğŸ“¦ ç¼“å­˜ç®¡ç†
        self.message_cache = {}
        self.vehicle_cache = defaultdict(dict)
        self.last_sent_timestamps = defaultdict(float)
        
        # ğŸ¯ é‡‡æ ·è¿‡æ»¤å™¨
        self.vehicle_message_counts = defaultdict(int)
        self.sampling_counters = defaultdict(int)
        
    async def initialize(self, websocket_manager):
        """åˆå§‹åŒ–ç¼“å­˜ä¼˜åŒ–å™¨"""
        # é¿å…é‡å¤åˆå§‹åŒ–
        if self._initialized:
            logger.debug("ç¼“å­˜ä¼˜åŒ–å™¨å·²åˆå§‹åŒ–ï¼Œè·³è¿‡é‡å¤åˆå§‹åŒ–")
            return True
            
        try:
            # è¿æ¥Redis
            self.redis_client = redis.from_url(
                self.redis_url,
                decode_responses=True,
                retry_on_timeout=True,
                health_check_interval=30,
                socket_keepalive=True
                # ç§»é™¤socket_keepalive_optionsä»¥é¿å…ç±»å‹é”™è¯¯
            )
            await self.redis_client.ping()
            
            self.websocket_manager = websocket_manager
            self.stats["start_time"] = time.time()
            
            # åˆ›å»ºä¼˜åŒ–åçš„æ¶ˆè´¹è€…ç»„
            await self._create_optimized_consumer_groups()
            
            # è®¾ç½®åˆå§‹åŒ–çŠ¶æ€
            self._initialized = True
            logger.info("âœ… Redis Streamç¼“å­˜ä¼˜åŒ–å™¨åˆå§‹åŒ–æˆåŠŸ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ç¼“å­˜ä¼˜åŒ–å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            # æ‰“å°æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯ï¼Œå¸®åŠ©è¯Šæ–­
            import traceback
            logger.debug(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            return False
    
    async def _create_optimized_consumer_groups(self):
        """åˆ›å»ºä¼˜åŒ–çš„æ¶ˆè´¹è€…ç»„"""
        optimized_streams = {
            "fault_diagnosis_results": "optimized_fault_group",
            "vehicle_health_assessments": "optimized_health_group",
            "cached_messages": "cached_message_group"  # æ–°å¢ç¼“å­˜æ¶ˆæ¯æµ
        }
        
        for stream_name, group_name in optimized_streams.items():
            try:
                await self.redis_client.xgroup_create(
                    stream_name, 
                    group_name, 
                    id="0", 
                    mkstream=True
                )
                logger.debug(f"ğŸ”§ åˆ›å»ºä¼˜åŒ–æ¶ˆè´¹è€…ç»„: {stream_name} -> {group_name}")
            except Exception as e:
                if "BUSYGROUP" not in str(e):
                    logger.warning(f"åˆ›å»ºæ¶ˆè´¹è€…ç»„å¤±è´¥ {stream_name}: {e}")

    async def start_optimized_monitoring(self):
        """å¯åŠ¨ä¼˜åŒ–çš„ç›‘æ§ç³»ç»Ÿ"""
        if not self.redis_client or not self.websocket_manager:
            logger.error("âŒ ç¼“å­˜ä¼˜åŒ–å™¨æœªåˆå§‹åŒ–")
            return
            
        # é˜²æ­¢é‡å¤å¯åŠ¨
        if self.is_running:
            logger.info("ğŸ“Š ç¼“å­˜ä¼˜åŒ–ç›‘æ§å·²åœ¨è¿è¡Œï¼Œè·³è¿‡é‡å¤å¯åŠ¨")
            return
            
        self.is_running = True
        logger.info("ğŸš€ å¯åŠ¨Redis Streamç¼“å­˜ä¼˜åŒ–ç›‘æ§...")
        
        # å¯åŠ¨å¤šä¸ªä¼˜åŒ–ä»»åŠ¡
        tasks = [
            # æ¶ˆæ¯æ¥æ”¶ä»»åŠ¡
            asyncio.create_task(self._optimized_message_receiver()),
            # æ¶ˆæ¯å¤„ç†ä»»åŠ¡
            asyncio.create_task(self._optimized_message_processor()),
            # é‡è¯•å¤„ç†ä»»åŠ¡
            asyncio.create_task(self._retry_processor()),
            # ç¼“å­˜æ¸…ç†ä»»åŠ¡
            asyncio.create_task(self._cache_cleaner()),
            # æ€§èƒ½ç›‘æ§ä»»åŠ¡
            asyncio.create_task(self._performance_monitor())
        ]
        
        try:
            await asyncio.gather(*tasks)
        except Exception as e:
            logger.error(f"âŒ ä¼˜åŒ–ç›‘æ§ä»»åŠ¡å¼‚å¸¸: {e}")
        finally:
            self.is_running = False

    async def _optimized_message_receiver(self):
        """ä¼˜åŒ–çš„æ¶ˆæ¯æ¥æ”¶å™¨ - å¤§æ‰¹é‡è¯»å–"""
        consumer_id = "cache_optimizer_receiver"
        
        while self.is_running:
            try:
                # ğŸš€ å¤§æ‰¹é‡è¯»å–æ¶ˆæ¯ï¼Œæé«˜ååé‡
                messages = await self.redis_client.xreadgroup(
                    "optimized_fault_group",
                    consumer_id,
                    {
                        "fault_diagnosis_results": ">",
                        "vehicle_health_assessments": ">"
                    },
                    count=50,  # å¢åŠ åˆ°50æ¡
                    block=500   # å‡å°‘é˜»å¡æ—¶é—´åˆ°500ms
                )
                
                for stream, msgs in messages:
                    for message_id, fields in msgs:
                        self.stats["total_received"] += 1
                        
                        # ğŸ§  æ™ºèƒ½è¿‡æ»¤å’Œé‡‡æ ·
                        if await self._should_process_message(fields):
                            try:
                                # éé˜»å¡åŠ å…¥å¤„ç†é˜Ÿåˆ—
                                self.processing_queue.put_nowait((stream, message_id, fields))
                            except asyncio.QueueFull:
                                # é˜Ÿåˆ—æ»¡æ—¶ï¼ŒåŠ å…¥é‡è¯•é˜Ÿåˆ—
                                try:
                                    self.retry_queue.put_nowait((stream, message_id, fields, time.time()))
                                    logger.warning("ğŸ“¦ å¤„ç†é˜Ÿåˆ—å·²æ»¡ï¼Œæ¶ˆæ¯åŠ å…¥é‡è¯•é˜Ÿåˆ—")
                                except asyncio.QueueFull:
                                    logger.error("âŒ é‡è¯•é˜Ÿåˆ—ä¹Ÿå·²æ»¡ï¼Œä¸¢å¼ƒæ¶ˆæ¯")
                                    self.stats["error_count"] += 1
                        
                        # ç¡®è®¤æ¶ˆæ¯å¤„ç†
                        await self.redis_client.xack(stream, "optimized_fault_group", message_id)
                        
            except Exception as e:
                logger.error(f"âŒ ä¼˜åŒ–æ¶ˆæ¯æ¥æ”¶å¤±è´¥: {e}")
                await asyncio.sleep(1)

    async def _should_process_message(self, fields: Dict) -> bool:
        """æ™ºèƒ½æ¶ˆæ¯è¿‡æ»¤å’Œé‡‡æ ·"""
        vehicle_id = fields.get("vehicle_id", "unknown")
        message_type = fields.get("message_type", "fault_diagnosis")
        
        # ğŸ¯ å¢åŠ æ¶ˆæ¯è®¡æ•°
        self.vehicle_message_counts[vehicle_id] += 1
        self.sampling_counters[vehicle_id] += 1
        
        # ğŸš¨ å…³é”®è­¦æŠ¥å§‹ç»ˆå¤„ç†
        if self._is_critical_alert(fields):
            return True
        
        # ğŸ”„ æ ¹æ®æ¶ˆæ¯é¢‘ç‡æ™ºèƒ½é‡‡æ ·
        message_count = self.vehicle_message_counts[vehicle_id]
        sampling_rate = self._get_sampling_rate(message_count)
        
        # é‡‡æ ·å†³ç­–
        if self.sampling_counters[vehicle_id] % int(1 / sampling_rate) == 0:
            self.stats["total_sampled"] += 1
            return True
        
        return False
    
    def _is_critical_alert(self, fields: Dict) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºå…³é”®è­¦æŠ¥"""
        try:
            # ä¿®å¤å­—ç¬¦ä¸²è½¬æ•´æ•°é—®é¢˜
            score_str = fields.get("score", "0")
            health_score_str = fields.get("health_score", "100")
            
            # å®‰å…¨è½¬æ¢ä¸ºæµ®ç‚¹æ•°
            score = float(score_str) if score_str and isinstance(score_str, (str, int, float)) else 0
            health_score = float(health_score_str) if health_score_str and isinstance(health_score_str, (str, int, float)) else 100
            
            # ä¸¥é‡æ•…éšœæˆ–å¥åº·åº¦è¿‡ä½
            if score > 80 or health_score < 30:
                return True
            
            # çŠ¶æ€ä¸ºå±é™©
            if fields.get("status") in ["danger", "critical", "fault"]:
                return True
                
        except (ValueError, TypeError) as e:
            logger.warning(f"è§£æè­¦æŠ¥æ•°å€¼å¤±è´¥: {e}, åŸå§‹å€¼: score={fields.get('score')}, health_score={fields.get('health_score')}")
            
        return False
    
    def _get_sampling_rate(self, message_count: int) -> float:
        """æ ¹æ®æ¶ˆæ¯é¢‘ç‡è·å–é‡‡æ ·ç‡"""
        if message_count > 1000:  # é«˜é¢‘
            return self.sampling_rates["high_frequency"]
        elif message_count > 100:  # ä¸­é¢‘
            return self.sampling_rates["medium_frequency"]
        else:  # ä½é¢‘
            return self.sampling_rates["low_frequency"]

    async def _optimized_message_processor(self):
        """ä¼˜åŒ–çš„æ¶ˆæ¯å¤„ç†å™¨ - æ‰¹é‡å¤„ç†"""
        batch = []
        last_process_time = time.time()
        
        while self.is_running:
            try:
                # ğŸš€ æ”¶é›†æ‰¹é‡æ¶ˆæ¯
                timeout = self.processing_interval
                try:
                    stream, message_id, fields = await asyncio.wait_for(
                        self.processing_queue.get(), timeout=timeout
                    )
                    batch.append((stream, message_id, fields))
                except asyncio.TimeoutError:
                    pass
                
                # æ‰¹é‡å¤„ç†æ¡ä»¶
                current_time = time.time()
                should_process = (
                    len(batch) >= self.max_batch_size or
                    (batch and current_time - last_process_time >= self.processing_interval)
                )
                
                if should_process and batch:
                    await self._process_message_batch(batch)
                    batch.clear()
                    last_process_time = current_time
                    
            except Exception as e:
                logger.error(f"âŒ ä¼˜åŒ–æ¶ˆæ¯å¤„ç†å¤±è´¥: {e}")
                await asyncio.sleep(0.1)

    async def _process_message_batch(self, batch: List):
        """æ‰¹é‡å¤„ç†æ¶ˆæ¯"""
        if not batch:
            return
            
        processed_count = 0
        frontend_messages = []
        
        for stream, message_id, fields in batch:
            try:
                # ğŸ”§ å¤„ç†æ¶ˆæ¯å¹¶ç¼“å­˜
                processed_message = await self._process_and_cache_message(fields)
                if processed_message:
                    frontend_messages.append(processed_message)
                    processed_count += 1
                    
            except Exception as e:
                logger.error(f"âŒ å¤„ç†å•æ¡æ¶ˆæ¯å¤±è´¥: {e}")
                self.stats["error_count"] += 1
        
        # ğŸš€ æ‰¹é‡å‘é€åˆ°å‰ç«¯
        if frontend_messages:
            await self._batch_send_to_frontend(frontend_messages)
        
        self.stats["total_processed"] += processed_count
        logger.debug(f"ğŸ“¦ æ‰¹é‡å¤„ç†å®Œæˆ: {processed_count}/{len(batch)} æ¡æ¶ˆæ¯")

    async def _process_and_cache_message(self, fields: Dict) -> Optional[Dict]:
        """å¤„ç†æ¶ˆæ¯å¹¶å®ç°æ™ºèƒ½ç¼“å­˜"""
        vehicle_id = fields.get("vehicle_id", "unknown")
        
        # ğŸ”„ æ£€æŸ¥ç¼“å­˜
        cache_key = f"{vehicle_id}:{fields.get('fault_type', 'unknown')}"
        cached_message = self.message_cache.get(cache_key)
        
        if cached_message:
            # ğŸ¯ ç¼“å­˜å‘½ä¸­ï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°
            last_update = cached_message.get("timestamp", 0)
            current_time = time.time()
            
            if current_time - last_update < 1:  # 1ç§’å†…ä¸é‡å¤å‘é€
                self.stats["cache_hits"] += 1
                return None
        
        self.stats["cache_misses"] += 1
        
        # ğŸ”§ æ„å»ºä¼˜åŒ–åçš„æ¶ˆæ¯
        try:
            # ä¿®å¤å­—ç¬¦ä¸²è½¬æ•´æ•°é—®é¢˜
            score = fields.get("score", "0")
            health_score = fields.get("health_score", "100")
            
            # å®‰å…¨è½¬æ¢ä¸ºæµ®ç‚¹æ•°
            score_float = float(score) if score and isinstance(score, (str, int, float)) else 0
            health_score_float = float(health_score) if health_score and isinstance(health_score, (str, int, float)) else 100
            
            optimized_message = {
                "vehicle_id": vehicle_id,
                "fault_type": fields.get("fault_type", "unknown"),
                "timestamp": datetime.now().isoformat(),
                "score": score_float,
                "health_score": health_score_float,
                "status": fields.get("status", "unknown"),
                "location": self._get_location_from_vehicle_id(vehicle_id),
                "processing_time": time.time(),
                "cache_optimized": True
            }
            
            # ğŸ“¦ ç¼“å­˜æ¶ˆæ¯
            self.message_cache[cache_key] = optimized_message.copy()
            self.stats["total_cached"] += 1
            
            # ğŸ”„ æ›´æ–°è½¦è¾†ç¼“å­˜
            self.vehicle_cache[vehicle_id].update({
                "last_update": time.time(),
                "message_count": self.vehicle_message_counts[vehicle_id],
                "last_score": optimized_message["score"],
                "last_health_score": optimized_message["health_score"]
            })
            
            return optimized_message
        except Exception as e:
            logger.error(f"âŒ å¤„ç†æ¶ˆæ¯æ ¼å¼å¤±è´¥: {e}, åŸå§‹å­—æ®µ: score={fields.get('score')}, health_score={fields.get('health_score')}")
            return None

    async def _batch_send_to_frontend(self, messages: List[Dict]):
        """æ‰¹é‡å‘é€æ¶ˆæ¯åˆ°å‰ç«¯"""
        if not messages or not self.websocket_manager:
            return
            
        try:
            # ğŸš€ æ‰¹é‡å‘é€ï¼Œå‡å°‘WebSocketè°ƒç”¨æ¬¡æ•°
            for message in messages:
                await self.websocket_manager.broadcast_to_frontends(message)
                
            logger.debug(f"ğŸ“¡ æ‰¹é‡å‘é€åˆ°å‰ç«¯: {len(messages)} æ¡æ¶ˆæ¯")
            
        except Exception as e:
            logger.error(f"âŒ æ‰¹é‡å‘é€åˆ°å‰ç«¯å¤±è´¥: {e}")
            # å‘é€å¤±è´¥çš„æ¶ˆæ¯åŠ å…¥é‡è¯•é˜Ÿåˆ—
            for message in messages:
                try:
                    self.retry_queue.put_nowait(("frontend", None, message, time.time()))
                except asyncio.QueueFull:
                    logger.error("âŒ é‡è¯•é˜Ÿåˆ—å·²æ»¡ï¼Œæ— æ³•é‡è¯•å‘é€")

    async def _retry_processor(self):
        """é‡è¯•å¤„ç†å™¨"""
        while self.is_running:
            try:
                # å¤„ç†é‡è¯•é˜Ÿåˆ—
                try:
                    stream, message_id, fields, retry_time = await asyncio.wait_for(
                        self.retry_queue.get(), timeout=1.0
                    )
                    
                    # æ£€æŸ¥é‡è¯•æ—¶é—´é—´éš”
                    if time.time() - retry_time > 5:  # 5ç§’åé‡è¯•
                        if stream == "frontend":
                            # é‡è¯•å‘é€åˆ°å‰ç«¯
                            await self.websocket_manager.broadcast_to_frontends(fields)
                        else:
                            # é‡è¯•å¤„ç†æ¶ˆæ¯
                            processed_message = await self._process_and_cache_message(fields)
                            if processed_message:
                                await self._batch_send_to_frontend([processed_message])
                        
                        self.stats["retry_count"] += 1
                        logger.debug("ğŸ”„ æ¶ˆæ¯é‡è¯•æˆåŠŸ")
                    else:
                        # é‡æ–°åŠ å…¥é‡è¯•é˜Ÿåˆ—
                        self.retry_queue.put_nowait((stream, message_id, fields, retry_time))
                        
                except asyncio.TimeoutError:
                    continue
                    
            except Exception as e:
                logger.error(f"âŒ é‡è¯•å¤„ç†å¤±è´¥: {e}")
                await asyncio.sleep(1)

    async def _cache_cleaner(self):
        """ç¼“å­˜æ¸…ç†å™¨"""
        while self.is_running:
            try:
                await asyncio.sleep(60)  # æ¯åˆ†é’Ÿæ¸…ç†ä¸€æ¬¡
                
                current_time = time.time()
                
                # ğŸ§¹ æ¸…ç†è¿‡æœŸç¼“å­˜
                expired_keys = []
                for key, message in self.message_cache.items():
                    if current_time - message.get("processing_time", 0) > self.cache_ttl:
                        expired_keys.append(key)
                
                for key in expired_keys:
                    del self.message_cache[key]
                
                # ğŸ§¹ æ¸…ç†è½¦è¾†ç¼“å­˜
                expired_vehicles = []
                for vehicle_id, cache_data in self.vehicle_cache.items():
                    if current_time - cache_data.get("last_update", 0) > self.cache_ttl:
                        expired_vehicles.append(vehicle_id)
                
                for vehicle_id in expired_vehicles:
                    del self.vehicle_cache[vehicle_id]
                    if vehicle_id in self.vehicle_message_counts:
                        del self.vehicle_message_counts[vehicle_id]
                    if vehicle_id in self.sampling_counters:
                        del self.sampling_counters[vehicle_id]
                
                if expired_keys or expired_vehicles:
                    logger.info(f"ğŸ§¹ ç¼“å­˜æ¸…ç†å®Œæˆ: æ¶ˆæ¯ç¼“å­˜-{len(expired_keys)}, è½¦è¾†ç¼“å­˜-{len(expired_vehicles)}")
                    
            except Exception as e:
                logger.error(f"âŒ ç¼“å­˜æ¸…ç†å¤±è´¥: {e}")

    async def _performance_monitor(self):
        """æ€§èƒ½ç›‘æ§å™¨"""
        while self.is_running:
            try:
                await asyncio.sleep(10)  # æ¯10ç§’ç»Ÿè®¡ä¸€æ¬¡
                
                current_time = time.time()
                if self.stats["start_time"]:
                    elapsed_time = current_time - self.stats["start_time"]
                    
                    # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
                    receive_rate = self.stats["total_received"] / elapsed_time
                    process_rate = self.stats["total_processed"] / elapsed_time
                    cache_hit_rate = (
                        self.stats["cache_hits"] / 
                        (self.stats["cache_hits"] + self.stats["cache_misses"])
                        if (self.stats["cache_hits"] + self.stats["cache_misses"]) > 0 else 0
                    )
                    
                    # è®¡ç®—æ¶ˆæ¯ä¸¢å¤±ç‡
                    total_input = self.stats["total_received"]
                    total_output = self.stats["total_processed"]
                    loss_rate = (
                        (total_input - total_output) / total_input 
                        if total_input > 0 else 0
                    )
                    
                    logger.info(
                        f"ğŸ“Š ç¼“å­˜ä¼˜åŒ–å™¨æ€§èƒ½ç»Ÿè®¡:\n"
                        f"   ğŸ“ˆ æ¥æ”¶é€Ÿç‡: {receive_rate:.1f} msg/s\n"
                        f"   âš¡ å¤„ç†é€Ÿç‡: {process_rate:.1f} msg/s\n"
                        f"   ğŸ’¾ ç¼“å­˜å‘½ä¸­ç‡: {cache_hit_rate:.1%}\n"
                        f"   ğŸ“‰ æ¶ˆæ¯ä¸¢å¤±ç‡: {loss_rate:.1%}\n"
                        f"   ğŸ”„ é‡è¯•æ¬¡æ•°: {self.stats['retry_count']}\n"
                        f"   ğŸ“¦ ç¼“å­˜æ¶ˆæ¯æ•°: {len(self.message_cache)}\n"
                        f"   ğŸš— æ´»è·ƒè½¦è¾†æ•°: {len(self.vehicle_cache)}"
                    )
                    
            except Exception as e:
                logger.error(f"âŒ æ€§èƒ½ç›‘æ§å¤±è´¥: {e}")

    def _get_location_from_vehicle_id(self, vehicle_id: str) -> str:
        """ä»è½¦è¾†IDè·å–ä½ç½®ä¿¡æ¯"""
        if not vehicle_id or vehicle_id == "unknown":
            return "æœªçŸ¥ä½ç½®"
        
        # ç®€åŒ–çš„ä½ç½®æ˜ å°„
        if "ç²¤B" in vehicle_id or "SEAL" in vehicle_id:
            return "æ·±åœ³ç¦ç”°åŒº"
        elif "é™•A" in vehicle_id:
            return "è¥¿å®‰é«˜æ–°åŒº"
        else:
            return "æœªçŸ¥ä½ç½®"

    async def stop_monitoring(self):
        """åœæ­¢ç›‘æ§"""
        self.is_running = False
        if self.redis_client:
            await self.redis_client.close()
        
        # æ¸…ç†ç¼“å­˜
        self.message_cache.clear()
        self.vehicle_cache.clear()
        
        logger.info("ğŸ›‘ Redis Streamç¼“å­˜ä¼˜åŒ–å™¨å·²åœæ­¢")

    def get_optimizer_stats(self) -> Dict[str, Any]:
        """è·å–ä¼˜åŒ–å™¨ç»Ÿè®¡ä¿¡æ¯"""
        current_time = time.time()
        elapsed_time = current_time - self.stats["start_time"] if self.stats["start_time"] else 0
        
        return {
            "is_running": self.is_running,
            "elapsed_time": elapsed_time,
            "total_received": self.stats["total_received"],
            "total_processed": self.stats["total_processed"],
            "total_cached": self.stats["total_cached"],
            "cache_hit_rate": (
                self.stats["cache_hits"] / 
                (self.stats["cache_hits"] + self.stats["cache_misses"])
                if (self.stats["cache_hits"] + self.stats["cache_misses"]) > 0 else 0
            ),
            "loss_rate": (
                (self.stats["total_received"] - self.stats["total_processed"]) / 
                self.stats["total_received"]
                if self.stats["total_received"] > 0 else 0
            ),
            "active_vehicles": len(self.vehicle_cache),
            "cached_messages": len(self.message_cache),
            "retry_count": self.stats["retry_count"],
            "error_count": self.stats["error_count"],
            "queue_sizes": {
                "processing": self.processing_queue.qsize(),
                "retry": self.retry_queue.qsize()
            }
        }

# å…¨å±€ç¼“å­˜ä¼˜åŒ–å™¨å®ä¾‹
stream_cache_optimizer = StreamCacheOptimizer() 