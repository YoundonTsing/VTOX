#!/usr/bin/env python3
"""
ğŸ§  Redis Stream è‡ªé€‚åº”æ¶ˆè´¹è€…ç®¡ç†å™¨
å®æ—¶ç›‘æ§ç³»ç»Ÿè´Ÿè½½ï¼Œæ™ºèƒ½è°ƒæ•´æ¶ˆè´¹è€…æ•°é‡ï¼Œæä¾›éä¾µå…¥å¼çš„è‡ªé€‚åº”æ‰©å±•èƒ½åŠ›
"""

import asyncio
import logging
import time
import json
import psutil
import redis.asyncio as redis
from typing import Dict, Any, List, Optional, Tuple
from datetime import datetime, timedelta
from dataclasses import dataclass, field
from collections import deque, defaultdict
import aiohttp
import numpy as np
from enum import Enum
import math

logger = logging.getLogger(__name__)

class ScalingAction(Enum):
    """æ‰©å±•è¡Œä¸ºæšä¸¾"""
    SCALE_UP = "scale_up"
    SCALE_DOWN = "scale_down"
    MAINTAIN = "maintain"
    EMERGENCY_STOP = "emergency_stop"

@dataclass
class SystemMetrics:
    """ç³»ç»ŸæŒ‡æ ‡"""
    timestamp: datetime
    # Redis StreamæŒ‡æ ‡
    stream_lengths: Dict[str, int] = field(default_factory=dict)
    consumer_lag: Dict[str, int] = field(default_factory=dict)
    processing_rate: Dict[str, float] = field(default_factory=dict)
    pending_messages: Dict[str, int] = field(default_factory=dict)
    
    # ç³»ç»Ÿèµ„æºæŒ‡æ ‡
    cpu_usage: float = 0.0
    memory_usage: float = 0.0
    memory_available_gb: float = 0.0
    
    # åº”ç”¨æŒ‡æ ‡
    active_consumers: Dict[str, int] = field(default_factory=dict)
    total_consumers: int = 0
    throughput: float = 0.0
    error_rate: float = 0.0
    
    # ç½‘ç»œæŒ‡æ ‡
    redis_latency_ms: float = 0.0
    api_response_time_ms: float = 0.0

@dataclass
class ScalingDecision:
    """æ‰©å±•å†³ç­–"""
    action: ScalingAction
    fault_type: str
    current_count: int
    target_count: int
    confidence: float  # 0-1ï¼Œå†³ç­–ç½®ä¿¡åº¦
    reasoning: List[str]  # å†³ç­–åŸå› 
    priority: int  # 1-10ï¼Œä¼˜å…ˆçº§
    estimated_impact: Dict[str, float]  # é¢„æœŸå½±å“

@dataclass
class AdaptiveConfig:
    """è‡ªé€‚åº”é…ç½®"""
    # ç›‘æ§é…ç½®
    monitoring_interval: int = 15  # ç›‘æ§é—´éš”ï¼ˆç§’ï¼‰
    metrics_history_size: int = 100  # å†å²æŒ‡æ ‡ä¿ç•™æ•°é‡
    
    # æ‰©å±•é˜ˆå€¼
    high_load_threshold: float = 0.8  # é«˜è´Ÿè½½é˜ˆå€¼
    low_load_threshold: float = 0.3   # ä½è´Ÿè½½é˜ˆå€¼
    cpu_safe_threshold: float = 70.0  # CPUå®‰å…¨é˜ˆå€¼
    memory_safe_threshold: float = 80.0  # å†…å­˜å®‰å…¨é˜ˆå€¼
    
    # æ‰©å±•ç­–ç•¥
    max_consumers_per_fault: int = 15  # æ¯ç§æ•…éšœæœ€å¤§æ¶ˆè´¹è€…æ•°
    min_consumers_per_fault: int = 2   # æ¯ç§æ•…éšœæœ€å°æ¶ˆè´¹è€…æ•°
    scale_step_size: int = 2           # æ¯æ¬¡æ‰©å±•æ­¥é•¿
    
    # å®‰å…¨æœºåˆ¶
    max_scale_operations_per_hour: int = 6  # æ¯å°æ—¶æœ€å¤§æ‰©å±•æ“ä½œæ•°
    cooldown_period_minutes: int = 10       # æ‰©å±•å†·å´æœŸ
    emergency_stop_error_rate: float = 0.1  # ç´§æ€¥åœæ­¢é”™è¯¯ç‡é˜ˆå€¼
    
    # é¢„æµ‹å‚æ•°
    prediction_window_minutes: int = 30     # é¢„æµ‹çª—å£
    trend_sensitivity: float = 0.7          # è¶‹åŠ¿æ•æ„Ÿåº¦

class AdaptiveConsumerManager:
    """è‡ªé€‚åº”æ¶ˆè´¹è€…ç®¡ç†å™¨"""
    
    def __init__(self, 
                 redis_url: str = "redis://localhost:6379",
                 api_base_url: str = "http://localhost:8000",
                 config: Optional[AdaptiveConfig] = None):
        self.redis_url = redis_url
        self.api_base_url = api_base_url
        self.config = config or AdaptiveConfig()
        
        # è¿æ¥å’Œè®¤è¯
        self.redis_client: Optional[redis.Redis] = None
        self.session: Optional[aiohttp.ClientSession] = None
        self.auth_token: Optional[str] = None
        
        # æ•°æ®å­˜å‚¨
        self.metrics_history: deque = deque(maxlen=self.config.metrics_history_size)
        self.scaling_history: List[ScalingDecision] = []
        self.last_scaling_time: Dict[str, datetime] = {}
        
        # è¿è¡ŒçŠ¶æ€
        self.is_running = False
        self.monitoring_task: Optional[asyncio.Task] = None
        
        # æ•…éšœç±»å‹æ˜ å°„
        self.fault_types = [
            "turn_fault", "insulation", "bearing", 
            "eccentricity", "broken_bar"
        ]
        
        # æ™ºèƒ½å†³ç­–å¼•æ“
        self.decision_engine = IntelligentDecisionEngine(self.config)
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            "total_scaling_operations": 0,
            "successful_scale_ups": 0,
            "successful_scale_downs": 0,
            "prevented_operations": 0,
            "emergency_stops": 0,
            "start_time": None
        }

    async def initialize(self) -> bool:
        """åˆå§‹åŒ–ç®¡ç†å™¨"""
        try:
            logger.info("ğŸ§  åˆå§‹åŒ–è‡ªé€‚åº”æ¶ˆè´¹è€…ç®¡ç†å™¨...")
            
            # åˆå§‹åŒ–Redisè¿æ¥
            self.redis_client = redis.from_url(self.redis_url)
            await self.redis_client.ping()
            
            # åˆå§‹åŒ–HTTPä¼šè¯
            self.session = aiohttp.ClientSession()
            
            # è®¤è¯
            if not await self._authenticate():
                logger.error("âŒ APIè®¤è¯å¤±è´¥")
                return False
            
            logger.info("âœ… è‡ªé€‚åº”ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
            return False

    async def start_adaptive_monitoring(self):
        """å¯åŠ¨è‡ªé€‚åº”ç›‘æ§"""
        if self.is_running:
            logger.warning("âš ï¸ è‡ªé€‚åº”ç›‘æ§å·²åœ¨è¿è¡Œä¸­")
            return
        
        self.is_running = True
        self.stats["start_time"] = datetime.now()
        
        logger.info("ğŸš€ å¯åŠ¨å®æ—¶è‡ªé€‚åº”æ‰©å±•ç›‘æ§")
        logger.info(f"ğŸ“Š ç›‘æ§é—´éš”: {self.config.monitoring_interval}ç§’")
        logger.info(f"ğŸ¯ ç›®æ ‡: æ™ºèƒ½ç»´æŒæœ€ä¼˜æ¶ˆè´¹è€…æ•°é‡")
        
        self.monitoring_task = asyncio.create_task(self._adaptive_monitoring_loop())

    async def stop_adaptive_monitoring(self):
        """åœæ­¢è‡ªé€‚åº”ç›‘æ§"""
        logger.info("ğŸ›‘ åœæ­¢è‡ªé€‚åº”ç›‘æ§...")
        
        self.is_running = False
        
        if self.monitoring_task and not self.monitoring_task.done():
            self.monitoring_task.cancel()
            try:
                await self.monitoring_task
            except asyncio.CancelledError:
                pass
        
        logger.info("âœ… è‡ªé€‚åº”ç›‘æ§å·²åœæ­¢")

    async def _adaptive_monitoring_loop(self):
        """è‡ªé€‚åº”ç›‘æ§ä¸»å¾ªç¯"""
        consecutive_errors = 0
        max_consecutive_errors = 5
        
        while self.is_running:
            try:
                # æ”¶é›†ç³»ç»ŸæŒ‡æ ‡
                metrics = await self._collect_system_metrics()
                if metrics:
                    self.metrics_history.append(metrics)
                    
                    # æ™ºèƒ½å†³ç­–
                    decisions = await self._make_scaling_decisions(metrics)
                    
                    # æ‰§è¡Œå†³ç­–
                    for decision in decisions:
                        if await self._should_execute_decision(decision):
                            await self._execute_scaling_decision(decision)
                    
                    consecutive_errors = 0
                else:
                    consecutive_errors += 1
                    
            except Exception as e:
                consecutive_errors += 1
                logger.error(f"âŒ ç›‘æ§å¾ªç¯å¼‚å¸¸: {e}")
                
                if consecutive_errors >= max_consecutive_errors:
                    logger.critical(f"ğŸ’€ è¿ç»­{max_consecutive_errors}æ¬¡é”™è¯¯ï¼Œåœæ­¢ç›‘æ§")
                    break
            
            await asyncio.sleep(self.config.monitoring_interval)

    async def _collect_system_metrics(self) -> Optional[SystemMetrics]:
        """æ”¶é›†ç³»ç»ŸæŒ‡æ ‡"""
        try:
            metrics = SystemMetrics(timestamp=datetime.now())
            
            # Redis StreamæŒ‡æ ‡
            await self._collect_redis_metrics(metrics)
            
            # ç³»ç»Ÿèµ„æºæŒ‡æ ‡
            await self._collect_system_resources(metrics)
            
            # åº”ç”¨æŒ‡æ ‡
            await self._collect_application_metrics(metrics)
            
            return metrics
            
        except Exception as e:
            logger.error(f"âŒ æ”¶é›†æŒ‡æ ‡å¤±è´¥: {e}")
            return None

    async def _collect_redis_metrics(self, metrics: SystemMetrics):
        """æ”¶é›†Redis StreamæŒ‡æ ‡"""
        try:
            # è·å–Streamé•¿åº¦
            stream_names = [
                "motor_raw_data", "fault_diagnosis_results", 
                "vehicle_health_assessments", "performance_metrics", "system_alerts"
            ]
            
            for stream_name in stream_names:
                try:
                    length = await self.redis_client.xlen(stream_name)
                    metrics.stream_lengths[stream_name] = length
                    
                    # è·å–æ¶ˆè´¹è€…ç»„ä¿¡æ¯
                    groups_info = await self.redis_client.xinfo_groups(stream_name)
                    for group_info in groups_info:
                        group_name = group_info['name'].decode() if isinstance(group_info['name'], bytes) else group_info['name']
                        pending = group_info['pending']
                        
                        # æå–æ•…éšœç±»å‹
                        fault_type = self._extract_fault_type_from_group(group_name)
                        if fault_type:
                            metrics.pending_messages[fault_type] = pending
                            
                except Exception as e:
                    logger.debug(f"è·³è¿‡Stream {stream_name}: {e}")
                    
        except Exception as e:
            logger.error(f"âŒ æ”¶é›†RedisæŒ‡æ ‡å¤±è´¥: {e}")

    async def _collect_system_resources(self, metrics: SystemMetrics):
        """æ”¶é›†ç³»ç»Ÿèµ„æºæŒ‡æ ‡"""
        try:
            # CPUä½¿ç”¨ç‡
            metrics.cpu_usage = psutil.cpu_percent(interval=1)
            
            # å†…å­˜ä½¿ç”¨ç‡
            memory = psutil.virtual_memory()
            metrics.memory_usage = memory.percent
            metrics.memory_available_gb = memory.available / (1024**3)
            
            # Rediså»¶è¿Ÿæµ‹è¯•
            start_time = time.time()
            await self.redis_client.ping()
            metrics.redis_latency_ms = (time.time() - start_time) * 1000
            
        except Exception as e:
            logger.error(f"âŒ æ”¶é›†ç³»ç»Ÿèµ„æºå¤±è´¥: {e}")

    async def _collect_application_metrics(self, metrics: SystemMetrics):
        """æ”¶é›†åº”ç”¨æŒ‡æ ‡"""
        try:
            # é€šè¿‡APIè·å–ç³»ç»ŸçŠ¶æ€
            async with self.session.get(
                f"{self.api_base_url}/api/v1/diagnosis-stream/system/status",
                headers=self._get_auth_headers()
            ) as response:
                if response.status == 200:
                    data = await response.json()
                    system_data = data.get("data", {})
                    
                    metrics.total_consumers = system_data.get("active_consumers", 0)
                    metrics.throughput = system_data.get("throughput", 0.0)
                    metrics.error_rate = system_data.get("error_rate", 0.0)
                    
                    # æŒ‰æ•…éšœç±»å‹ç»Ÿè®¡æ¶ˆè´¹è€…æ•°é‡
                    for fault_type in self.fault_types:
                        # ä¼°ç®—æ¯ç§æ•…éšœç±»å‹çš„æ¶ˆè´¹è€…æ•°é‡ï¼ˆåŸºäºæ€»æ•°å¹³å‡åˆ†é…ï¼‰
                        metrics.active_consumers[fault_type] = metrics.total_consumers // len(self.fault_types)
                        
        except Exception as e:
            logger.debug(f"æ”¶é›†åº”ç”¨æŒ‡æ ‡å¤±è´¥: {e}")

    async def _make_scaling_decisions(self, current_metrics: SystemMetrics) -> List[ScalingDecision]:
        """åˆ¶å®šæ‰©å±•å†³ç­–"""
        decisions = []
        
        try:
            for fault_type in self.fault_types:
                decision = await self.decision_engine.analyze_fault_type(
                    fault_type, current_metrics, self.metrics_history
                )
                
                if decision and decision.action != ScalingAction.MAINTAIN:
                    decisions.append(decision)
                    
        except Exception as e:
            logger.error(f"âŒ åˆ¶å®šå†³ç­–å¤±è´¥: {e}")
        
        # æŒ‰ä¼˜å…ˆçº§æ’åº
        decisions.sort(key=lambda x: x.priority, reverse=True)
        
        return decisions

    async def _should_execute_decision(self, decision: ScalingDecision) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥æ‰§è¡Œå†³ç­–"""
        try:
            # æ£€æŸ¥å†·å´æœŸ
            last_scale_time = self.last_scaling_time.get(decision.fault_type)
            if last_scale_time:
                time_since_last = datetime.now() - last_scale_time
                if time_since_last.total_seconds() < self.config.cooldown_period_minutes * 60:
                    logger.info(f"â³ {decision.fault_type} åœ¨å†·å´æœŸä¸­ï¼Œè·³è¿‡æ‰©å±•")
                    self.stats["prevented_operations"] += 1
                    return False
            
            # æ£€æŸ¥æ“ä½œé¢‘ç‡é™åˆ¶
            recent_operations = len([
                d for d in self.scaling_history 
                if hasattr(d, 'timestamp') and (datetime.now() - d.timestamp) < timedelta(hours=1)
            ])
            
            if recent_operations >= self.config.max_scale_operations_per_hour:
                logger.warning(f"âš ï¸ è¾¾åˆ°æ¯å°æ—¶æœ€å¤§æ“ä½œæ•°é™åˆ¶: {recent_operations}")
                self.stats["prevented_operations"] += 1
                return False
            
            # æ£€æŸ¥ç½®ä¿¡åº¦
            if decision.confidence < 0.6:  # ç½®ä¿¡åº¦é˜ˆå€¼
                logger.info(f"ğŸ“Š {decision.fault_type} å†³ç­–ç½®ä¿¡åº¦è¿‡ä½: {decision.confidence:.2f}")
                return False
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ å†³ç­–æ£€æŸ¥å¤±è´¥: {e}")
            return False

    async def _execute_scaling_decision(self, decision: ScalingDecision):
        """æ‰§è¡Œæ‰©å±•å†³ç­–"""
        try:
            logger.info(f"ğŸ¯ æ‰§è¡Œæ‰©å±•å†³ç­–: {decision.fault_type} {decision.action.value}")
            logger.info(f"   ğŸ“Š å½“å‰: {decision.current_count} â†’ ç›®æ ‡: {decision.target_count}")
            logger.info(f"   ğŸ§  ç½®ä¿¡åº¦: {decision.confidence:.2f}")
            logger.info(f"   ğŸ’­ åŸå› : {', '.join(decision.reasoning)}")
            
            # è°ƒç”¨ç°æœ‰çš„æ‰©å±•API
            success = await self._call_scaling_api(decision.fault_type, decision.target_count)
            
            if success:
                # è®°å½•æˆåŠŸæ“ä½œ
                self.last_scaling_time[decision.fault_type] = datetime.now()
                self.scaling_history.append(decision)
                self.stats["total_scaling_operations"] += 1
                
                if decision.action == ScalingAction.SCALE_UP:
                    self.stats["successful_scale_ups"] += 1
                elif decision.action == ScalingAction.SCALE_DOWN:
                    self.stats["successful_scale_downs"] += 1
                
                logger.info(f"âœ… {decision.fault_type} æ‰©å±•æˆåŠŸ")
                
                # ç­‰å¾…ä¸€æ®µæ—¶é—´è®©å˜æ›´ç”Ÿæ•ˆ
                await asyncio.sleep(5)
                
            else:
                logger.error(f"âŒ {decision.fault_type} æ‰©å±•å¤±è´¥")
                
        except Exception as e:
            logger.error(f"âŒ æ‰§è¡Œæ‰©å±•å†³ç­–å¼‚å¸¸: {e}")

    async def _call_scaling_api(self, fault_type: str, target_count: int) -> bool:
        """è°ƒç”¨æ‰©å±•API"""
        try:
            async with self.session.post(
                f"{self.api_base_url}/api/v1/diagnosis-stream/system/scale",
                params={
                    "fault_type": fault_type,
                    "new_count": target_count
                },
                headers=self._get_auth_headers()
            ) as response:
                return response.status == 200
                
        except Exception as e:
            logger.error(f"âŒ APIè°ƒç”¨å¤±è´¥: {e}")
            return False

    async def _authenticate(self) -> bool:
        """APIè®¤è¯"""
        try:
            async with self.session.post(
                f"{self.api_base_url}/auth/token",
                json={"username": "user1", "password": "password123"}
            ) as response:
                if response.status == 200:
                    result = await response.json()
                    self.auth_token = result.get("access_token")
                    return True
                return False
        except:
            return False

    def _get_auth_headers(self) -> Dict[str, str]:
        """è·å–è®¤è¯å¤´"""
        return {"Authorization": f"Bearer {self.auth_token}"} if self.auth_token else {}

    def _extract_fault_type_from_group(self, group_name: str) -> Optional[str]:
        """ä»æ¶ˆè´¹è€…ç»„åç§°æå–æ•…éšœç±»å‹"""
        for fault_type in self.fault_types:
            if fault_type in group_name:
                return fault_type
        return None

    async def get_adaptive_stats(self) -> Dict[str, Any]:
        """è·å–è‡ªé€‚åº”ç®¡ç†ç»Ÿè®¡"""
        uptime = (datetime.now() - self.stats["start_time"]).total_seconds() if self.stats["start_time"] else 0
        
        return {
            "status": "running" if self.is_running else "stopped",
            "uptime_seconds": uptime,
            "config": {
                "monitoring_interval": self.config.monitoring_interval,
                "max_consumers_per_fault": self.config.max_consumers_per_fault,
                "cpu_safe_threshold": self.config.cpu_safe_threshold,
                "memory_safe_threshold": self.config.memory_safe_threshold
            },
            "statistics": self.stats,
            "recent_metrics": {
                "total_samples": len(self.metrics_history),
                "latest_cpu_usage": self.metrics_history[-1].cpu_usage if self.metrics_history else 0,
                "latest_memory_usage": self.metrics_history[-1].memory_usage if self.metrics_history else 0,
                "latest_throughput": self.metrics_history[-1].throughput if self.metrics_history else 0
            },
            "recent_decisions": [
                {
                    "fault_type": d.fault_type,
                    "action": d.action.value,
                    "confidence": d.confidence,
                    "reasoning": d.reasoning
                }
                for d in self.scaling_history[-10:]  # æœ€è¿‘10ä¸ªå†³ç­–
            ]
        }

    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        try:
            await self.stop_adaptive_monitoring()
            
            if self.redis_client:
                await self.redis_client.close()
            
            if self.session:
                await self.session.close()
            
            logger.info("âœ… è‡ªé€‚åº”ç®¡ç†å™¨èµ„æºæ¸…ç†å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ èµ„æºæ¸…ç†å¤±è´¥: {e}")


class IntelligentDecisionEngine:
    """æ™ºèƒ½å†³ç­–å¼•æ“"""
    
    def __init__(self, config: AdaptiveConfig):
        self.config = config

    async def analyze_fault_type(self, 
                               fault_type: str, 
                               current_metrics: SystemMetrics,
                               history: deque) -> Optional[ScalingDecision]:
        """åˆ†æç‰¹å®šæ•…éšœç±»å‹çš„æ‰©å±•éœ€æ±‚"""
        try:
            current_consumers = current_metrics.active_consumers.get(fault_type, 2)
            pending_messages = current_metrics.pending_messages.get(fault_type, 0)
            
            # è®¡ç®—è´Ÿè½½æŒ‡æ ‡
            load_factor = self._calculate_load_factor(current_metrics, fault_type)
            
            # è¶‹åŠ¿åˆ†æ
            trend = self._analyze_trend(history, fault_type)
            
            # èµ„æºæ£€æŸ¥
            resource_availability = self._check_resource_availability(current_metrics)
            
            # å†³ç­–é€»è¾‘
            action, target_count, confidence, reasoning = self._make_decision(
                fault_type, current_consumers, load_factor, trend, 
                resource_availability, pending_messages
            )
            
            if action == ScalingAction.MAINTAIN:
                return None
            
            return ScalingDecision(
                action=action,
                fault_type=fault_type,
                current_count=current_consumers,
                target_count=target_count,
                confidence=confidence,
                reasoning=reasoning,
                priority=self._calculate_priority(action, load_factor, pending_messages),
                estimated_impact=self._estimate_impact(action, current_consumers, target_count)
            )
            
        except Exception as e:
            logger.error(f"âŒ åˆ†ææ•…éšœç±»å‹ {fault_type} å¤±è´¥: {e}")
            return None

    def _calculate_load_factor(self, metrics: SystemMetrics, fault_type: str) -> float:
        """è®¡ç®—è´Ÿè½½å› å­"""
        try:
            pending = metrics.pending_messages.get(fault_type, 0)
            consumers = max(metrics.active_consumers.get(fault_type, 1), 1)
            
            # åŸºäºå¾…å¤„ç†æ¶ˆæ¯æ•°å’Œæ¶ˆè´¹è€…æ•°è®¡ç®—è´Ÿè½½
            base_load = pending / consumers if consumers > 0 else 0
            
            # è€ƒè™‘ç³»ç»Ÿæ•´ä½“ååé‡
            throughput_factor = min(metrics.throughput / 1000, 1.0)  # æ ‡å‡†åŒ–åˆ°0-1
            
            # ç»¼åˆè´Ÿè½½å› å­
            load_factor = (base_load * 0.7 + throughput_factor * 0.3) / 100
            
            return min(load_factor, 1.0)
            
        except Exception:
            return 0.5  # é»˜è®¤ä¸­ç­‰è´Ÿè½½

    def _analyze_trend(self, history: deque, fault_type: str) -> float:
        """åˆ†æè¶‹åŠ¿ï¼ˆæ­£å€¼è¡¨ç¤ºä¸Šå‡è¶‹åŠ¿ï¼Œè´Ÿå€¼è¡¨ç¤ºä¸‹é™è¶‹åŠ¿ï¼‰"""
        try:
            if len(history) < 5:
                return 0.0
            
            # å–æœ€è¿‘çš„æ•°æ®ç‚¹
            recent_data = list(history)[-10:]
            pending_values = [
                m.pending_messages.get(fault_type, 0) for m in recent_data
            ]
            
            if len(pending_values) < 2:
                return 0.0
            
            # ç®€å•çº¿æ€§è¶‹åŠ¿è®¡ç®—
            x = np.arange(len(pending_values))
            y = np.array(pending_values)
            
            if len(x) > 1:
                slope = np.polyfit(x, y, 1)[0]
                return float(slope)
            
            return 0.0
            
        except Exception:
            return 0.0

    def _check_resource_availability(self, metrics: SystemMetrics) -> Dict[str, bool]:
        """æ£€æŸ¥èµ„æºå¯ç”¨æ€§"""
        return {
            "cpu_available": metrics.cpu_usage < self.config.cpu_safe_threshold,
            "memory_available": metrics.memory_usage < self.config.memory_safe_threshold,
            "redis_responsive": metrics.redis_latency_ms < 100,  # 100msé˜ˆå€¼
        }

    def _make_decision(self, 
                      fault_type: str,
                      current_consumers: int,
                      load_factor: float,
                      trend: float,
                      resource_availability: Dict[str, bool],
                      pending_messages: int) -> Tuple[ScalingAction, int, float, List[str]]:
        """åˆ¶å®šå†³ç­–"""
        
        reasoning = []
        confidence = 0.5
        
        # ç´§æ€¥æƒ…å†µæ£€æŸ¥
        if pending_messages > 10000:  # ç´§æ€¥é˜ˆå€¼
            reasoning.append(f"ç´§æ€¥æƒ…å†µï¼šå¾…å¤„ç†æ¶ˆæ¯æ•°è¿‡é«˜ ({pending_messages})")
            if all(resource_availability.values()) and current_consumers < self.config.max_consumers_per_fault:
                target = min(current_consumers + self.config.scale_step_size * 2, self.config.max_consumers_per_fault)
                return ScalingAction.SCALE_UP, target, 0.9, reasoning
        
        # æ‰©å±•å†³ç­–
        if load_factor > self.config.high_load_threshold:
            reasoning.append(f"é«˜è´Ÿè½½æ£€æµ‹ (è´Ÿè½½å› å­: {load_factor:.2f})")
            confidence += 0.3
            
            if trend > 0:
                reasoning.append(f"ä¸Šå‡è¶‹åŠ¿æ£€æµ‹ (è¶‹åŠ¿: {trend:.2f})")
                confidence += 0.2
            
            if all(resource_availability.values()):
                reasoning.append("ç³»ç»Ÿèµ„æºå……è¶³")
                confidence += 0.2
            else:
                reasoning.append("ç³»ç»Ÿèµ„æºä¸è¶³ï¼Œé™åˆ¶æ‰©å±•")
                confidence -= 0.3
            
            if current_consumers < self.config.max_consumers_per_fault and confidence > 0.6:
                target = min(current_consumers + self.config.scale_step_size, self.config.max_consumers_per_fault)
                return ScalingAction.SCALE_UP, target, confidence, reasoning
        
        # ç¼©å‡å†³ç­–
        elif load_factor < self.config.low_load_threshold:
            reasoning.append(f"ä½è´Ÿè½½æ£€æµ‹ (è´Ÿè½½å› å­: {load_factor:.2f})")
            confidence += 0.3
            
            if trend < 0:
                reasoning.append(f"ä¸‹é™è¶‹åŠ¿æ£€æµ‹ (è¶‹åŠ¿: {trend:.2f})")
                confidence += 0.2
            
            if current_consumers > self.config.min_consumers_per_fault and confidence > 0.6:
                target = max(current_consumers - 1, self.config.min_consumers_per_fault)
                return ScalingAction.SCALE_DOWN, target, confidence, reasoning
        
        # ç»´æŒç°çŠ¶
        return ScalingAction.MAINTAIN, current_consumers, 0.8, ["è´Ÿè½½æ­£å¸¸ï¼Œç»´æŒå½“å‰é…ç½®"]

    def _calculate_priority(self, action: ScalingAction, load_factor: float, pending_messages: int) -> int:
        """è®¡ç®—ä¼˜å…ˆçº§"""
        base_priority = 5
        
        if action == ScalingAction.SCALE_UP:
            # æ‰©å±•ä¼˜å…ˆçº§åŸºäºè´Ÿè½½å’Œå¾…å¤„ç†æ¶ˆæ¯
            priority = base_priority + int(load_factor * 3) + min(pending_messages // 1000, 2)
        elif action == ScalingAction.SCALE_DOWN:
            # ç¼©å‡ä¼˜å…ˆçº§è¾ƒä½
            priority = max(base_priority - 2, 1)
        else:
            priority = base_priority
        
        return min(priority, 10)

    def _estimate_impact(self, action: ScalingAction, current: int, target: int) -> Dict[str, float]:
        """ä¼°ç®—å½±å“"""
        if action == ScalingAction.SCALE_UP:
            improvement_ratio = target / max(current, 1)
            return {
                "throughput_improvement": (improvement_ratio - 1) * 0.8,  # 80%æ•ˆç‡
                "latency_reduction": (improvement_ratio - 1) * 0.6,
                "resource_increase": (target - current) * 0.1  # æ¯ä¸ªæ¶ˆè´¹è€…çº¦10%èµ„æº
            }
        elif action == ScalingAction.SCALE_DOWN:
            reduction_ratio = current / max(target, 1)
            return {
                "throughput_reduction": (reduction_ratio - 1) * 0.8,
                "resource_savings": (current - target) * 0.1
            }
        
        return {}


# å…¨å±€å®ä¾‹
adaptive_consumer_manager = AdaptiveConsumerManager() 