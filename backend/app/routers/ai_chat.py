from fastapi import APIRouter, HTTPException, status
from fastapi.responses import JSONResponse, StreamingResponse
from pydantic import BaseModel
import logging
import aiohttp
import asyncio
import json
import os
from typing import List, Dict, Any, Optional
import random

# å¯¼å…¥RAGæœåŠ¡
from ..services.rag import VtoxRAGEngine, VtoxRAGConfig

router = APIRouter(
    prefix="/ai",
    tags=["ai_chat"]
)

logger = logging.getLogger(__name__)

# åˆå§‹åŒ–RAGå¼•æ“
rag_engine = None

async def get_rag_engine():
    """è·å–RAGå¼•æ“å®ä¾‹"""
    global rag_engine
    if rag_engine is None:
        # ä½¿ç”¨ç‹¬ç«‹ç¯å¢ƒé…ç½®
        rag_config = VtoxRAGConfig(isolated_rag_env=True)
        rag_engine = VtoxRAGEngine(rag_config)
        await rag_engine.initialize()
    return rag_engine

# AI èŠå¤©è¯·æ±‚æ¨¡å‹
class ChatMessage(BaseModel):
    role: str  # "user" æˆ– "assistant"
    content: str

class ChatRequest(BaseModel):
    message: str
    conversation_history: Optional[List[ChatMessage]] = []
    model_provider: Optional[str] = "huggingface"  # "huggingface", "ollama", "openai"

class ChatResponse(BaseModel):
    response: str
    model_used: str
    processing_time: float

# é…ç½®ä¸åŒçš„ AI æ¨¡å‹æä¾›å•†
AI_PROVIDERS = {
    "qwen": {
        "url": "https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation",
        "headers": {
            "Authorization": f"Bearer {os.getenv('QWEN_API_KEY', '')}",
            "Content-Type": "application/json"
        }
    },
    "huggingface": {
        "url": "https://api-inference.huggingface.co/models/microsoft/DialoGPT-medium",
        "headers": {
            "Authorization": f"Bearer {os.getenv('HUGGINGFACE_API_KEY', '')}",
            "Content-Type": "application/json"
        }
    },
    "openai_compatible": {
        "url": "https://api.openai.com/v1/chat/completions",
        "headers": {
            "Authorization": f"Bearer {os.getenv('OPENAI_API_KEY', '')}",
            "Content-Type": "application/json"
        }
    }
}

# MCSA ä¸“ä¸šçŸ¥è¯†åº“
MCSA_KNOWLEDGE = {
    "è½´æ‰¿æ•…éšœ": {
        "outer_race": "å¤–åœˆæ•…éšœç‰¹å¾é¢‘ç‡: BPFO = (N/2) * (1 - d/D * cos(Ï†)) * fs",
        "inner_race": "å†…åœˆæ•…éšœç‰¹å¾é¢‘ç‡: BPFI = (N/2) * (1 + d/D * cos(Ï†)) * fs", 
        "ball": "æ»šåŠ¨ä½“æ•…éšœç‰¹å¾é¢‘ç‡: BSF = (D/2d) * (1 - (d/D * cos(Ï†))Â²) * fs",
        "cage": "ä¿æŒæ¶æ•…éšœç‰¹å¾é¢‘ç‡: FTF = (1/2) * (1 - d/D * cos(Ï†)) * fs"
    },
    "åå¿ƒæ•…éšœ": {
        "static": "é™åå¿ƒ: åœ¨ç”µæºé¢‘ç‡å¤„å‡ºç°è¾¹é¢‘å¸¦",
        "dynamic": "åŠ¨åå¿ƒ: åœ¨è½¬å­é¢‘ç‡å¤„å‡ºç°è°ƒåˆ¶"
    },
    "æ–­æ¡æ•…éšœ": {
        "frequency": "æ–­æ¡æ•…éšœé¢‘ç‡: fb = f * (1 Â± 2*s), sä¸ºè½¬å·®ç‡"
    },
    "åŒé—´çŸ­è·¯": {
        "frequency": "åŒé—´çŸ­è·¯å¯¼è‡´è´Ÿåºç”µæµå¢å¤§ï¼Œå‡ºç°ç‰¹å¾é¢‘ç‡åˆ†é‡"
    }
}

def get_mcsa_response(user_message: str) -> str:
    """åŸºäº MCSA çŸ¥è¯†åº“ç”Ÿæˆå›å¤"""
    user_message_lower = user_message.lower()
    
    # å…³é”®è¯åŒ¹é…
    if any(keyword in user_message_lower for keyword in ["è½´æ‰¿", "bearing"]):
        if "å¤–åœˆ" in user_message_lower:
            return f"æ ¹æ®MCSAåˆ†æï¼Œè½´æ‰¿å¤–åœˆæ•…éšœçš„ç‰¹å¾é¢‘ç‡ä¸ºï¼š{MCSA_KNOWLEDGE['è½´æ‰¿æ•…éšœ']['outer_race']}ã€‚å»ºè®®æ£€æŸ¥é¢‘è°±ä¸­æ˜¯å¦å‡ºç°BPFOåŠå…¶è°æ³¢æˆåˆ†ã€‚å¦‚å‘ç°å¼‚å¸¸ï¼Œåº”ç«‹å³å®‰æ’åœæœºæ£€ä¿®ã€‚"
        elif "å†…åœˆ" in user_message_lower:
            return f"è½´æ‰¿å†…åœˆæ•…éšœç‰¹å¾ï¼š{MCSA_KNOWLEDGE['è½´æ‰¿æ•…éšœ']['inner_race']}ã€‚å†…åœˆæ•…éšœé€šå¸¸æ¯”å¤–åœˆæ•…éšœæ›´ä¸¥é‡ï¼Œéœ€è¦ç´§æ€¥å¤„ç†ã€‚"
        else:
            return "è½´æ‰¿æ•…éšœè¯Šæ–­è¦ç‚¹ï¼š1ï¼‰åˆ†æBPFOã€BPFIã€BSFã€FTFé¢‘ç‡æˆåˆ†ï¼›2ï¼‰è§‚å¯Ÿé¢‘è°±åŒ…ç»œï¼›3ï¼‰æ£€æŸ¥æŒ¯åŠ¨å¹…å€¼å˜åŒ–ï¼›4ï¼‰ç»“åˆæ¸©åº¦ç›‘æµ‹æ•°æ®ã€‚éœ€è¦å…·ä½“åˆ†æå“ªç§è½´æ‰¿æ•…éšœï¼Ÿ"
    
    elif any(keyword in user_message_lower for keyword in ["åå¿ƒ", "eccentricity"]):
        if "é™" in user_message_lower:
            return f"é™åå¿ƒæ•…éšœç‰¹å¾ï¼š{MCSA_KNOWLEDGE['åå¿ƒæ•…éšœ']['static']}ã€‚éœ€æ£€æŸ¥å®šå­ä¸è½¬å­ä¹‹é—´çš„æ°”éš™æ˜¯å¦å‡åŒ€ã€‚"
        elif "åŠ¨" in user_message_lower:
            return f"åŠ¨åå¿ƒæ•…éšœç‰¹å¾ï¼š{MCSA_KNOWLEDGE['åå¿ƒæ•…éšœ']['dynamic']}ã€‚å¯èƒ½æ˜¯è½´æ‰¿ç£¨æŸæˆ–å®‰è£…ä¸å½“é€ æˆã€‚"
        else:
            return "åå¿ƒæ•…éšœåˆ†ä¸ºé™åå¿ƒå’ŒåŠ¨åå¿ƒä¸¤ç§ã€‚é™åå¿ƒè¡¨ç°ä¸ºç”µæºé¢‘ç‡å¤„çš„è¾¹é¢‘å¸¦ï¼ŒåŠ¨åå¿ƒè¡¨ç°ä¸ºè½¬å­é¢‘ç‡è°ƒåˆ¶ã€‚è¯·æè¿°å…·ä½“çš„é¢‘è°±ç‰¹å¾ã€‚"
    
    elif any(keyword in user_message_lower for keyword in ["æ–­æ¡", "broken", "bar"]):
        return f"è½¬å­æ–­æ¡æ•…éšœç‰¹å¾ï¼š{MCSA_KNOWLEDGE['æ–­æ¡æ•…éšœ']['frequency']}ã€‚æ–­æ¡ä¼šå¯¼è‡´è½¬å­ç”µé˜»ä¸å¹³è¡¡ï¼Œäº§ç”Ÿç‰¹å¾é¢‘ç‡çš„è¾¹é¢‘å¸¦ã€‚å»ºè®®åˆ†æè´Ÿè½½å˜åŒ–æ—¶çš„é¢‘è°±æ¼”å˜ã€‚"
    
    elif any(keyword in user_message_lower for keyword in ["åŒé—´", "turn", "çŸ­è·¯"]):
        return f"åŒé—´çŸ­è·¯æ•…éšœï¼š{MCSA_KNOWLEDGE['åŒé—´çŸ­è·¯']['frequency']}ã€‚ä¼šå¼•èµ·ä¸‰ç›¸ç”µæµä¸å¹³è¡¡ï¼Œå¢å¤§è´Ÿåºç”µæµåˆ†é‡ã€‚å»ºè®®è¿›è¡Œè´Ÿåºé˜»æŠ—æµ‹è¯•ã€‚"
    
    elif any(keyword in user_message_lower for keyword in ["é¢‘è°±", "spectrum", "fft"]):
        return "MCSAé¢‘è°±åˆ†æè¦ç‚¹ï¼š1ï¼‰é€‰æ‹©åˆé€‚çš„é‡‡æ ·é¢‘ç‡ï¼›2ï¼‰ç¡®ä¿è¶³å¤Ÿçš„é¢‘ç‡åˆ†è¾¨ç‡ï¼›3ï¼‰ä½¿ç”¨çª—å‡½æ•°å‡å°‘æ³„æ¼ï¼›4ï¼‰åˆ†æåŸºé¢‘ã€è°æ³¢å’Œè¾¹é¢‘å¸¦ã€‚éœ€è¦åˆ†æä»€ä¹ˆç±»å‹çš„é¢‘è°±ç‰¹å¾ï¼Ÿ"
    
    elif any(keyword in user_message_lower for keyword in ["ç»´æŠ¤", "maintenance", "ä¿®å¤"]):
        return "åŸºäºMCSAè¯Šæ–­ç»“æœçš„ç»´æŠ¤å»ºè®®ï¼š1ï¼‰è½»å¾®å¼‚å¸¸ï¼šå¢åŠ ç›‘æµ‹é¢‘æ¬¡ï¼›2ï¼‰ä¸­ç­‰å¼‚å¸¸ï¼šè®¡åˆ’æ€§ç»´æŠ¤ï¼›3ï¼‰ä¸¥é‡å¼‚å¸¸ï¼šç«‹å³åœæœºæ£€ä¿®ã€‚å…·ä½“ç»´æŠ¤æ–¹æ¡ˆéœ€è¦ç»“åˆæ•…éšœç±»å‹å’Œè®¾å¤‡é‡è¦æ€§åˆ¶å®šã€‚"
    
    else:
        responses = [
            "æˆ‘æ˜¯MCSAæ™ºèƒ½è¯Šæ–­åŠ©æ‰‹ï¼Œä¸“æ³¨äºç”µæœºæ•…éšœåˆ†æã€‚è¯·æè¿°æ‚¨é‡åˆ°çš„å…·ä½“é—®é¢˜ï¼Œæ¯”å¦‚å¼‚å¸¸é¢‘è°±ã€æŒ¯åŠ¨ã€æ¸©åº¦ç­‰ç°è±¡ã€‚",
            "æ ¹æ®MCSAåˆ†æç»éªŒï¼Œè¯·æä¾›æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼šç”µæœºç±»å‹ã€è¿è¡Œå·¥å†µã€æ•…éšœç°è±¡ç­‰ï¼Œæˆ‘å°†ä¸ºæ‚¨æä¾›ä¸“ä¸šçš„è¯Šæ–­å»ºè®®ã€‚",
            "MCSAè¯Šæ–­å¯ä»¥æ£€æµ‹è½´æ‰¿æ•…éšœã€åå¿ƒã€æ–­æ¡ã€åŒé—´çŸ­è·¯ç­‰é—®é¢˜ã€‚è¯·å‘Šè¯‰æˆ‘æ‚¨éœ€è¦åˆ†æå“ªç§æ•…éšœç±»å‹ï¼Ÿ"
        ]
        return random.choice(responses)

async def call_huggingface_api(messages: List[Dict], api_key: Optional[str]) -> str:
    """è°ƒç”¨ Hugging Face API"""
    if not api_key:
        # å¦‚æœæ²¡æœ‰ API keyï¼Œä½¿ç”¨æœ¬åœ°çŸ¥è¯†åº“
        last_message = messages[-1]["content"] if messages else ""
        return get_mcsa_response(last_message)
    
    try:
        async with aiohttp.ClientSession() as session:
            # æ„å»ºå¯¹è¯ä¸Šä¸‹æ–‡
            conversation_text = ""
            for msg in messages[-5:]:  # åªå–æœ€è¿‘5è½®å¯¹è¯
                role = "User" if msg["role"] == "user" else "Assistant"
                conversation_text += f"{role}: {msg['content']}\n"
            
            payload = {
                "inputs": conversation_text,
                "parameters": {
                    "max_length": 200,
                    "temperature": 0.7,
                    "return_full_text": False
                }
            }
            
            headers = AI_PROVIDERS["huggingface"]["headers"].copy()
            headers["Authorization"] = f"Bearer {api_key}"
            
            async with session.post(
                AI_PROVIDERS["huggingface"]["url"],
                json=payload,
                headers=headers,
                timeout=aiohttp.ClientTimeout(total=30)
            ) as response:
                if response.status == 200:
                    result = await response.json()
                    if isinstance(result, list) and len(result) > 0:
                        generated_text = result[0].get("generated_text", "")
                        # ç»“åˆ MCSA çŸ¥è¯†è¿›è¡Œå›å¤
                        mcsa_response = get_mcsa_response(messages[-1]["content"])
                        return f"{mcsa_response}\n\nè¡¥å……ä¿¡æ¯ï¼š{generated_text}" if generated_text else mcsa_response
                    else:
                        return get_mcsa_response(messages[-1]["content"])
                else:
                    logger.warning(f"Hugging Face API error: {response.status}")
                    return get_mcsa_response(messages[-1]["content"])
                    
    except Exception as e:
        logger.error(f"Error calling Hugging Face API: {str(e)}")
        return get_mcsa_response(messages[-1]["content"])

async def call_qwen_api_stream(messages: List[Dict], api_key: Optional[str]):
    """è°ƒç”¨é˜¿é‡Œäº‘é€šä¹‰åƒé—® API - æµå¼å“åº”ç‰ˆæœ¬"""
    if not api_key:
        # è¿”å›æœ¬åœ°MCSAçŸ¥è¯†åº“å›å¤çš„ç”Ÿæˆå™¨
        response = get_mcsa_response(messages[-1]["content"] if messages else "")
        for char in response:
            yield f"data: {json.dumps({'content': char}, ensure_ascii=False)}\n\n"
            await asyncio.sleep(0.03)  # æ¨¡æ‹Ÿæ‰“å­—æ•ˆæœ
        yield f"data: {json.dumps({'done': True}, ensure_ascii=False)}\n\n"
        return
    
    try:
        async with aiohttp.ClientSession() as session:
            # æ·»åŠ ç³»ç»Ÿæç¤º
            system_message = {
                "role": "system",
                "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç”µæœºæ•…éšœè¯Šæ–­ä¸“å®¶ï¼Œä¸“æ³¨äºMCSAï¼ˆç”µæœºç”µæµä¿¡å·åˆ†æï¼‰æŠ€æœ¯ã€‚è¯·åŸºäºä¸“ä¸šçŸ¥è¯†ä¸ºç”¨æˆ·æä¾›å‡†ç¡®çš„æ•…éšœè¯Šæ–­å’Œç»´æŠ¤å»ºè®®ã€‚å›å¤ä½¿ç”¨Markdownæ ¼å¼ï¼ŒåŒ…å«é€‚å½“çš„æ ‡é¢˜ã€åˆ—è¡¨ã€è¡¨æ ¼å’Œä»£ç å—ã€‚"
            }
            
            api_messages = [system_message] + messages[-10:]  # æœ€è¿‘10è½®å¯¹è¯
            
            payload = {
                "model": "qwen-plus",
                "input": {
                    "messages": api_messages
                },
                "parameters": {
                    "result_format": "message",
                    "temperature": 0.7,
                    "max_tokens": 2000,
                    "incremental_output": True  # å¯ç”¨å¢é‡è¾“å‡ºï¼ˆæµå¼ï¼‰
                }
            }
            
            headers = {
                "Authorization": f"Bearer {api_key}",
                "Content-Type": "application/json",
                "Accept": "text/event-stream",
                "Cache-Control": "no-cache"
            }
            
            async with session.post(
                "https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation",
                json=payload,
                headers=headers,
                timeout=aiohttp.ClientTimeout(total=60)
            ) as response:
                if response.status == 200:
                    # å¤„ç†æµå¼å“åº”
                    buffer = ""
                    async for chunk in response.content.iter_chunked(1024):
                        if chunk:
                            buffer += chunk.decode('utf-8')
                            lines = buffer.split('\n')
                            buffer = lines[-1]  # ä¿ç•™æœªå®Œæˆçš„è¡Œ
                            
                            for line in lines[:-1]:
                                if line.strip().startswith('data:'):
                                    try:
                                        json_data = line.strip()[5:].strip()
                                        if json_data and json_data != '[DONE]':
                                            data = json.loads(json_data)
                                            if "output" in data and "choices" in data["output"]:
                                                content = data["output"]["choices"][0]["message"]["content"]
                                                yield f"data: {json.dumps({'content': content}, ensure_ascii=False)}\n\n"
                                    except json.JSONDecodeError:
                                        continue
                    
                    yield f"data: {json.dumps({'done': True}, ensure_ascii=False)}\n\n"
                else:
                    # APIè°ƒç”¨å¤±è´¥ï¼Œè¿”å›æœ¬åœ°å›å¤
                    response_text = get_mcsa_response(messages[-1]["content"] if messages else "")
                    for char in response_text:
                        yield f"data: {json.dumps({'content': char}, ensure_ascii=False)}\n\n"
                        await asyncio.sleep(0.03)
                    yield f"data: {json.dumps({'done': True}, ensure_ascii=False)}\n\n"
                    
    except Exception as e:
        logger.error(f"Error calling é€šä¹‰åƒé—® API: {str(e)}")
        # è¿”å›æœ¬åœ°å›å¤
        response_text = get_mcsa_response(messages[-1]["content"] if messages else "")
        for char in response_text:
            yield f"data: {json.dumps({'content': char}, ensure_ascii=False)}\n\n"
            await asyncio.sleep(0.03)
        yield f"data: {json.dumps({'done': True}, ensure_ascii=False)}\n\n"

async def call_qwen_api(messages: List[Dict], api_key: Optional[str]) -> str:
    """è°ƒç”¨é˜¿é‡Œäº‘é€šä¹‰åƒé—® API - é›†æˆRAGåŠŸèƒ½"""
    if not api_key:
        last_message = messages[-1]["content"] if messages else ""
        return get_mcsa_response(last_message)
    
    try:
        # å°è¯•ä½¿ç”¨RAGå¢å¼ºå›å¤
        try:
            rag_engine = await get_rag_engine()
            last_message = messages[-1]["content"] if messages else ""
            
            # ä½¿ç”¨RAGç”Ÿæˆå¢å¼ºå›å¤
            async def llm_func(prompt):
                async with aiohttp.ClientSession() as session:
                    system_message = {
                        "role": "system",
                        "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç”µæœºæ•…éšœè¯Šæ–­ä¸“å®¶ï¼Œä¸“æ³¨äºMCSAï¼ˆç”µæœºç”µæµä¿¡å·åˆ†æï¼‰æŠ€æœ¯ã€‚è¯·åŸºäºä¸“ä¸šçŸ¥è¯†ä¸ºç”¨æˆ·æä¾›å‡†ç¡®çš„æ•…éšœè¯Šæ–­å’Œç»´æŠ¤å»ºè®®ã€‚"
                    }
                    
                    api_messages = [system_message, {"role": "user", "content": prompt}]
                    
                    payload = {
                        "model": "qwen-plus",
                        "input": {
                            "messages": api_messages
                        },
                        "parameters": {
                            "result_format": "message",
                            "temperature": 0.7,
                            "max_tokens": 1500
                        }
                    }
                    
                    headers = {
                        "Authorization": f"Bearer {api_key}",
                        "Content-Type": "application/json"
                    }
                    
                    async with session.post(
                        "https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation",
                        json=payload,
                        headers=headers,
                        timeout=aiohttp.ClientTimeout(total=30)
                    ) as response:
                        if response.status == 200:
                            result = await response.json()
                            if "output" in result and "choices" in result["output"]:
                                return result["output"]["choices"][0]["message"]["content"]
                        return "æ— æ³•ç”Ÿæˆå›å¤"
            
            # ä½¿ç”¨RAGå¢å¼ºå›å¤
            rag_response = await rag_engine.generate_rag_response(last_message, llm_func)
            return rag_response
            
        except Exception as e:
            logger.warning(f"RAGåŠŸèƒ½ä¸å¯ç”¨ï¼Œé™çº§åˆ°æ ‡å‡†APIè°ƒç”¨: {e}")
    
        # æ ‡å‡†APIè°ƒç”¨ï¼ˆé™çº§æ–¹æ¡ˆï¼‰
        async with aiohttp.ClientSession() as session:
            # æ·»åŠ ç³»ç»Ÿæç¤º
            system_message = {
                "role": "system",
                "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç”µæœºæ•…éšœè¯Šæ–­ä¸“å®¶ï¼Œä¸“æ³¨äºMCSAï¼ˆç”µæœºç”µæµä¿¡å·åˆ†æï¼‰æŠ€æœ¯ã€‚è¯·åŸºäºä¸“ä¸šçŸ¥è¯†ä¸ºç”¨æˆ·æä¾›å‡†ç¡®çš„æ•…éšœè¯Šæ–­å’Œç»´æŠ¤å»ºè®®ã€‚"
            }
            
            api_messages = [system_message] + messages[-10:]  # æœ€è¿‘10è½®å¯¹è¯
            
            payload = {
                "model": "qwen-plus",
                "input": {
                    "messages": api_messages
                },
                "parameters": {
                    "result_format": "message",
                    "temperature": 0.7,
                    "max_tokens": 1500
                }
            }
            
            headers = {
                "Authorization": f"Bearer {api_key}",
                "Content-Type": "application/json"
            }
            
            async with session.post(
                "https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation",
                json=payload,
                headers=headers,
                timeout=aiohttp.ClientTimeout(total=30)
            ) as response:
                if response.status == 200:
                    result = await response.json()
                    if "output" in result and "choices" in result["output"]:
                        return result["output"]["choices"][0]["message"]["content"]
                    else:
                        return get_mcsa_response(messages[-1]["content"])
                else:
                    logger.warning(f"é€šä¹‰åƒé—® API error: {response.status}")
                    return get_mcsa_response(messages[-1]["content"])
                    
    except Exception as e:
        logger.error(f"Error calling é€šä¹‰åƒé—® API: {str(e)}")
        return get_mcsa_response(messages[-1]["content"])

async def call_openai_api(messages: List[Dict], api_key: Optional[str]) -> str:
    """è°ƒç”¨ OpenAI API"""
    if not api_key:
        last_message = messages[-1]["content"] if messages else ""
        return get_mcsa_response(last_message)
    
    try:
        async with aiohttp.ClientSession() as session:
            # æ·»åŠ ç³»ç»Ÿæç¤º
            system_message = {
                "role": "system",
                "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç”µæœºæ•…éšœè¯Šæ–­ä¸“å®¶ï¼Œä¸“æ³¨äºMCSAï¼ˆç”µæœºç”µæµä¿¡å·åˆ†æï¼‰æŠ€æœ¯ã€‚è¯·åŸºäºä¸“ä¸šçŸ¥è¯†ä¸ºç”¨æˆ·æä¾›å‡†ç¡®çš„æ•…éšœè¯Šæ–­å’Œç»´æŠ¤å»ºè®®ã€‚"
            }
            
            api_messages = [system_message] + messages[-10:]  # æœ€è¿‘10è½®å¯¹è¯
            
            payload = {
                "model": "gpt-3.5-turbo",
                "messages": api_messages,
                "max_tokens": 300,
                "temperature": 0.7
            }
            
            headers = AI_PROVIDERS["openai_compatible"]["headers"].copy()
            headers["Authorization"] = f"Bearer {api_key}"
            
            async with session.post(
                AI_PROVIDERS["openai_compatible"]["url"],
                json=payload,
                headers=headers,
                timeout=aiohttp.ClientTimeout(total=30)
            ) as response:
                if response.status == 200:
                    result = await response.json()
                    return result["choices"][0]["message"]["content"]
                else:
                    logger.warning(f"OpenAI API error: {response.status}")
                    return get_mcsa_response(messages[-1]["content"])
                    
    except Exception as e:
        logger.error(f"Error calling OpenAI API: {str(e)}")
        return get_mcsa_response(messages[-1]["content"])

@router.post("/chat", response_model=ChatResponse)
async def chat_with_ai(request: ChatRequest):
    """AI èŠå¤©æ¥å£"""
    start_time = asyncio.get_event_loop().time()
    
    try:
        # æ„å»ºæ¶ˆæ¯å†å²
        messages = []
        # æ£€æŸ¥conversation_historyæ˜¯å¦ä¸ºNone
        if request.conversation_history:
            for msg in request.conversation_history:
                messages.append({
                    "role": msg.role,
                    "content": msg.content
                })
        
        # æ·»åŠ å½“å‰ç”¨æˆ·æ¶ˆæ¯
        messages.append({
            "role": "user", 
            "content": request.message
        })
        
        # æ ¹æ®æä¾›å•†é€‰æ‹© API
        provider = (request.model_provider or "local").lower()
        response_text = ""
        
        if provider == "qwen":
            api_key = os.getenv('QWEN_API_KEY') or ""
            response_text = await call_qwen_api(messages, api_key)
            model_used = "è‡ªç ”æŠ€å·¥å¤§æ¨¡å‹LLM"
        elif provider == "openai":
            api_key = os.getenv('OPENAI_API_KEY') or ""
            response_text = await call_openai_api(messages, api_key)
            model_used = "OpenAI GPT-3.5"
        elif provider == "huggingface":
            api_key = os.getenv('HUGGINGFACE_API_KEY') or ""
            response_text = await call_huggingface_api(messages, api_key)
            model_used = "Hugging Face DialoGPT"
        else:
            # é»˜è®¤ä½¿ç”¨æœ¬åœ° MCSA çŸ¥è¯†åº“
            response_text = get_mcsa_response(request.message)
            model_used = "æœ¬åœ° MCSA çŸ¥è¯†åº“"
        
        processing_time = asyncio.get_event_loop().time() - start_time
        
        return ChatResponse(
            response=response_text,
            model_used=model_used,
            processing_time=round(processing_time * 1000, 2)  # è½¬æ¢ä¸ºæ¯«ç§’
        )
        
    except Exception as e:
        logger.error(f"Error in chat endpoint: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"èŠå¤©æœåŠ¡æš‚æ—¶ä¸å¯ç”¨: {str(e)}"
        )

@router.post("/chat/stream", summary="æµå¼AIèŠå¤©")
async def chat_stream(request: ChatRequest):
    """
    æµå¼AIèŠå¤©æ¥å£
    æ”¯æŒServer-Sent Events (SSE)æµå¼å“åº”
    """
    try:
        # æ„å»ºæ¶ˆæ¯å†å²
        messages = []
        # æ£€æŸ¥conversation_historyæ˜¯å¦ä¸ºNone
        if request.conversation_history:
            for msg in request.conversation_history:
                messages.append({
                    "role": msg.role,
                    "content": msg.content
                })
        
        # æ·»åŠ å½“å‰ç”¨æˆ·æ¶ˆæ¯
        messages.append({
            "role": "user", 
            "content": request.message
        })
        
        # æ ¹æ®æä¾›å•†é€‰æ‹©æµå¼API
        provider = (request.model_provider or "local").lower()
        
        if provider == "qwen":
            api_key = os.getenv('QWEN_API_KEY') or ""
            return StreamingResponse(
                call_qwen_api_stream(messages, api_key),
                media_type="text/event-stream",
                headers={
                    "Cache-Control": "no-cache",
                    "Connection": "keep-alive",
                    "Access-Control-Allow-Origin": "*",
                    "Access-Control-Allow-Headers": "*",
                }
            )
        else:
            # å¯¹äºå…¶ä»–æä¾›å•†ï¼Œæš‚æ—¶è¿”å›æœ¬åœ°å›å¤çš„æµå¼ç‰ˆæœ¬
            async def stream_local_response():
                response_text = get_mcsa_response(request.message)
                for char in response_text:
                    yield f"data: {json.dumps({'content': char}, ensure_ascii=False)}\n\n"
                    await asyncio.sleep(0.03)
                yield f"data: {json.dumps({'done': True}, ensure_ascii=False)}\n\n"
            
            return StreamingResponse(
                stream_local_response(),
                media_type="text/event-stream",
                headers={
                    "Cache-Control": "no-cache",
                    "Connection": "keep-alive",
                    "Access-Control-Allow-Origin": "*",
                    "Access-Control-Allow-Headers": "*",
                }
            )
    
    except Exception as e:
        logger.error(f"Error in stream chat endpoint: {str(e)}")
        
        # é”™è¯¯æ—¶è¿”å›æµå¼é”™è¯¯å›å¤
        async def stream_error_response():
            error_msg = f"æŠ±æ­‰ï¼ŒAIæœåŠ¡æš‚æ—¶ä¸å¯ç”¨ã€‚é”™è¯¯ä¿¡æ¯ï¼š{str(e)}ã€‚\n\nä½œä¸ºå¤‡é€‰ï¼Œæˆ‘å¯ä»¥åŸºäºMCSAçŸ¥è¯†åº“ä¸ºæ‚¨æä¾›åŸºç¡€çš„è¯Šæ–­å»ºè®®ã€‚"
            for char in error_msg:
                yield f"data: {json.dumps({'content': char}, ensure_ascii=False)}\n\n"
                await asyncio.sleep(0.03)
            yield f"data: {json.dumps({'done': True}, ensure_ascii=False)}\n\n"
        
        return StreamingResponse(
            stream_error_response(),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "Access-Control-Allow-Origin": "*",
                "Access-Control-Allow-Headers": "*",
            }
        )

@router.get("/models")
async def get_available_models():
    """è·å–å¯ç”¨çš„ AI æ¨¡å‹åˆ—è¡¨"""
    models = [
        {
            "id": "local",
            "name": "æœ¬åœ° MCSA çŸ¥è¯†åº“",
            "description": "åŸºäºä¸“ä¸š MCSA çŸ¥è¯†çš„æœ¬åœ°æ¨¡å‹ï¼Œå®Œå…¨å…è´¹",
            "status": "available",
            "free": True
        }
    ]
    
    # æ£€æŸ¥å„ç§ API key æ˜¯å¦é…ç½®
    if os.getenv('QWEN_API_KEY'):
        models.append({
            "id": "qwen",
            "name": "Jigong LLM",
            "description": "ä¸“ä¸ºä¸­æ–‡ä¼˜åŒ–çš„å¤§è¯­è¨€æ¨¡å‹ï¼Œæ¯æœˆ100ä¸‡tokenså…è´¹",
            "status": "available",
            "free": True
        })
    
    if os.getenv('HUGGINGFACE_API_KEY'):
        models.append({
            "id": "huggingface",
            "name": "Hugging Face DialoGPT",
            "description": "å…è´¹çš„å¯¹è¯æ¨¡å‹ï¼Œé€‚åˆæ—¥å¸¸äº¤æµ",
            "status": "available",
            "free": True
        })
    
    if os.getenv('OPENAI_API_KEY'):
        models.append({
            "id": "openai",
            "name": "OpenAI GPT-3.5",
            "description": "å¼ºå¤§çš„å¤§è¯­è¨€æ¨¡å‹ï¼Œæœ‰å…è´¹é¢åº¦",
            "status": "available", 
            "free": False
        })
    
    return {"models": models}

@router.get("/health")
async def health_check():
    """AI æœåŠ¡å¥åº·æ£€æŸ¥"""
    return {
        "status": "healthy",
        "service": "MCSA AI Chat Service",
        "timestamp": asyncio.get_event_loop().time()
    }

@router.post("/rag/query")
async def rag_query(request: ChatRequest):
    """
    RAGçŸ¥è¯†æ£€ç´¢æ¥å£
    ç›´æ¥ä½¿ç”¨RAGå¼•æ“è¿›è¡ŒçŸ¥è¯†æ£€ç´¢ï¼Œä¸è°ƒç”¨å¤§æ¨¡å‹
    """
    try:
        rag_engine = await get_rag_engine()
        
        # æ£€ç´¢ç›¸å…³çŸ¥è¯†
        relevant_docs = await rag_engine.query(request.message)
        
        if not relevant_docs:
            return {
                "response": "æœªæ‰¾åˆ°ç›¸å…³çš„MCSAçŸ¥è¯†åº“å†…å®¹ã€‚è¯·å°è¯•ä½¿ç”¨æ›´å…·ä½“çš„å…³é”®è¯ã€‚",
                "knowledge_sources": [],
                "model_used": "RAGçŸ¥è¯†æ£€ç´¢"
            }
        
        # æ„å»ºçŸ¥è¯†æºå›å¤
        response_parts = []
        knowledge_sources = []
        
        for doc in relevant_docs:
            response_parts.append(f"**{doc['title']}**\n{doc['content'][:300]}...")
            knowledge_sources.append({
                "title": doc['title'],
                "category": doc.get('category', ''),
                "score": doc.get('score', 0),
                "keywords": doc.get('keywords', [])
            })
        
        response_text = "\n\n".join(response_parts)
        response_text += "\n\nğŸ“š ä»¥ä¸Šå†…å®¹æ¥æºäºMCSAä¸“ä¸šçŸ¥è¯†åº“"
        
        return {
            "response": response_text,
            "knowledge_sources": knowledge_sources,
            "model_used": "RAGçŸ¥è¯†æ£€ç´¢"
        }
        
    except Exception as e:
        logger.error(f"RAGæŸ¥è¯¢å¤±è´¥: {str(e)}")
        return {
            "response": f"RAGçŸ¥è¯†æ£€ç´¢æœåŠ¡æš‚æ—¶ä¸å¯ç”¨: {str(e)}",
            "knowledge_sources": [],
            "model_used": "RAGçŸ¥è¯†æ£€ç´¢"
        }

@router.get("/rag/status")
async def rag_status():
    """è·å–RAGç³»ç»ŸçŠ¶æ€"""
    try:
        rag_engine = await get_rag_engine()
        return {
            "status": "active" if rag_engine.initialized else "initializing",
            "knowledge_count": len(rag_engine.knowledge_cache),
            "config": {
                "working_dir": rag_engine.config.working_dir,
                "top_k": rag_engine.config.top_k,
                "similarity_threshold": rag_engine.config.similarity_threshold
            },
            "knowledge_categories": list(set(
                knowledge.get('category', 'é€šç”¨') 
                for knowledge in rag_engine.knowledge_cache.values()
            ))
        }
    except Exception as e:
        return {
            "status": "error",
            "error": str(e)
        } 